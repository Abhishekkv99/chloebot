{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MoveData.ipynb\n",
      "importing Jupyter notebook from Encoder.ipynb\n",
      "importing Jupyter notebook from Elements.ipynb\n",
      "importing Jupyter notebook from Decoder.ipynb\n",
      "importing Jupyter notebook from LearningDynamics.ipynb\n"
     ]
    }
   ],
   "source": [
    "import time, sys\n",
    "#sys.path.append('/path/to/env/lib/python3.6/site-packages')\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import Options, json2datatools, num_batches, nopeak_mask, create_masks\n",
    "from Encoder import Encoder\n",
    "from Decoder import Decoder\n",
    "from LearningDynamics import CosineWithRestarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you are running this on a GPU ready environment or not, the below if else statement will set `device` appropriately to leverage the GPU or CPU. We are working on such a small dataset that for demonstration purposes, a CPU will do just as well and GPU is not usefull unless you have augmented the data to be significantly larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU for evaluation and training\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"you have\", torch.cuda.device_count(), \"GPUs\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print('using CPU for evaluation and training')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in START_HERE.ipynb you have already seen the `Options` class and `json2datatools` function. As a reminder, `opt` is just a way to keep and pass all your preferences in a single input, rather than type out every hyperparameter again and again. we will need the input and output vocabulary `infield, outfield` for our demonstration of how a sequence of words is represented by the transformer and we will use `data_iter` to show how data flows from the dataset though our transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options(batchsize=2, device=device, epochs=25, lr=0.01, \n",
    "              beam_width=3, max_len = 25, save_path = '../saved/weights/model_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../saved/pairs.json', opt=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look inside the Transformer, like older sequence to sequence models, Transformers also encode the sentence into vector representations and pass those representations along to the decoder to generate the response/reply/output/translation/etc. The Encoder and Decoder have subcomponents which is discussed in Elements, Encoder and Decoder. For now, just define the Transformer class, instantiate a model as chloe and define the optimizer and scheduler by running the cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "    def forward(self, src_seq, trg_seq, src_mask, trg_mask):\n",
    "        e_output = self.encoder(src_seq, src_mask)\n",
    "        d_output = self.decoder(trg_seq, e_output, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "    \n",
    "emb_dim, n_layers, heads, dropout = 32, 3, 8, 0.01 \n",
    "chloe = Transformer(len(infield.vocab), len(outfield.vocab), emb_dim, n_layers, heads, dropout)\n",
    "chloe.load_state_dict(torch.load(opt.save_path))\n",
    "\n",
    "if opt.device == torch.device(\"cuda:0\"):\n",
    "    chloe =  chloe.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = CosineWithRestarts(optimizer, T_max=num_batches(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line `batch = next(iter(data_iter))` extracts one batch of data into the variable `batch`. The batch has two parts to it, the `listen` part that is the input to the model, and the `reply` part that is the preferred output response to the corresponding `listen` input. This is a flawed assumption for conversation since there are many valid responses to anything one listens to or hears. But for now we will use this method to teach chloe to reply in some linguistically coherent manner. \n",
    "\n",
    "`.transpose(0,1)` is used to flip the orientation of the data contained in batch.listen\n",
    "\n",
    "the first dimension of `listen.shape` and `reply.shape` should match whatever you put into `opt = Options(batchsize=2` above, it is the number of samples in each batch. Each sample is one line in our pairs.json dataset. The next dimension is the sequence length. Although it seems messy, I have printed out each piece of data from the batch and it's shape. Each is a 2-dimensional tensor, or matrix. Above the listen and reply tensors I have printed the entire vocabulary so you can resolve that the integers represent coherent sentences. \n",
    "\n",
    "Notice that the reply always starts with an integer that represents the \"Start Of Sentence\" or `<sos>` token, and ends with the \"End Of Sentence\"or `<eos>` token, so it's sequence length is the length of the reply sentence + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x12d1e00f0>>, {'<unk>': 0, '<pad>': 1, '?': 2, 'you': 3, 'chloe': 4, 'i': 5, 'ok': 6, 'are': 7, 'hi': 8, 'a': 9, 'bye': 10, 'haha': 11, 'hello': 12, 'how': 13, 'later': 14, 'lol': 15, 'sure': 16, 'alive': 17, 'am': 18, 'any': 19, 'do': 20, 'dont': 21, 'dunno': 22, 'go': 23, 'gotta': 24, 'ill': 25, 'im': 26, 'joke': 27, 'know': 28, 'me': 29, 'more': 30, 'robot': 31, 'see': 32, 'talk': 33, 'tell': 34, 'think': 35, 'to': 36, 'true': 37, 'ttyl': 38, 'vicki': 39, 'what': 40, 'who': 41})\n",
      " ------------------------------------------------------ \n",
      "tensor([[34, 29,  9, 27,  4,  1],\n",
      "        [40, 20,  3, 35,  4,  2]]) torch.Size([2, 6])\n",
      " ------------------------------------------------------ \n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x12d1e0080>>, {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3, 'you': 4, '?': 5, ',': 6, 'i': 7, 'are': 8, 'joke': 9, 'say': 10, 'a': 11, 'bye': 12, 'cats': 13, 'do': 14, 'french': 15, 'hi': 16, 'how': 17, 'thank': 18, 'ttyl': 19, 'can': 20, 'for': 21, 'tell': 22, 'thanks': 23, 'alive': 24, 'and': 25, 'at': 26, 'beaucoup': 27, 'but': 28, 'dont': 29, 'either': 30, 'know': 31, 'laughing': 32, 'me': 33, 'meowci': 34, 'my': 35, 'ok': 36, 'some': 37, 'they': 38, 'to': 39, 'am': 40, 'biological': 41, 'bragging': 42, 'chloe': 43, 'conceding': 44, 'definition': 45, 'depends': 46, 'flash': 47, 'have': 48, 'just': 49, 'less': 50, 'math': 51, 'more': 52, 'of': 53, 'on': 54, 'robot': 55, 'sad': 56, 'science': 57, 'teach': 58, 'vicki': 59, 'viruses': 60, 'when': 61, 'will': 62, 'yes': 63, 'your': 64})\n",
      " ------------------------------------------------------ \n",
      "tensor([[ 2, 17, 14, 15, 13, 10, 18,  4,  5,  3,  1],\n",
      "        [ 2, 50, 42, 25, 47,  6, 52, 57, 25, 51,  3]]) torch.Size([2, 11])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(data_iter))\n",
    "listen = batch.listen.transpose(0,1)\n",
    "reply = batch.reply.transpose(0,1)\n",
    "reply_input = reply[:, :-1]\n",
    "\n",
    "print(infield.vocab.stoi)\n",
    "print(\" ------------------------------------------------------ \")\n",
    "print(listen, listen.shape)\n",
    "print(\" ------------------------------------------------------ \")\n",
    "print(outfield.vocab.stoi)\n",
    "print(\" ------------------------------------------------------ \")\n",
    "print(reply, reply.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have to be single values (scalars), lines (vectors), rectangles (matrices) or boxes. This means that if I want my batch to have sentence of unequal length, the shorter sentences have to be padded with the `<pad>` token so that every sentence is the same length, the example input batch here has been padded into a 3x3 tensor \n",
    "\n",
    "`i`      `love`   `you`\n",
    "\n",
    "`hello`  `<pad>` `<pad>`\n",
    "\n",
    "`see`  `ya` `<pad>`\n",
    "\n",
    "\n",
    "However, the padding does not carry useful information, so it must be masked away. Masks are tensors that are the same shape as the tensors they are a mask for, here we show a mask for the listen tensor and for the reply tensor. Think of a real mask, usually there will be two holes poked out for the eyes and one hole for the mouth, but otherwise the mask is meant to cover the rest of my face. The mask for the input listen tensor covers the padded elements. Seems tedious, but this is an artifact of the way wee need to prepare data to train efficiently. The mask tensors printed below have a `True` or `1` in the positions that are open for use, and a `False` or `0` in the positions that are meant to be covered or not used. \n",
    "\n",
    "The shape of the listen mask is **(batch_size, 1, input_sequence_length)**, why is there a 1 in there? We will explain later. \n",
    "\n",
    "The Mask for the output sequence, or reply tensor, has an additional meaning. Transformers use something called \"Attention\", which we will talk about in the Elements.ipynb, Encoder.ipynb and Decoder.ipynb sections. At a high level, attention is a good word to describe what is occuring computationally. Suppose you are replying to this phrase:\n",
    "\n",
    "\"action potentials flow from dendrite to soma to where ?\"\n",
    "\n",
    "you write:\n",
    "\n",
    "\"they from from soma to axon\"\n",
    "\n",
    "As you, the decoder, intend to write the words \"they flow from\", you are paying attention to the words \"action potentials flow from\" from the encoder, this is called encoder attention. In addition you see that instead of flow I wrote \"from\" twice becaue I didnt pay enough attention to what I had already written, This is called self attention. \n",
    "\n",
    "As the decoder writes each word in responce, one word at a time, the word it just produce and it's previous words are fed back into the decoder to get the next word so it can pay attention to what has already been written. \n",
    "\n",
    "During training, we judge it based on whether it produced the right word or not, if it does not, we insert the right word at each position and hope that the next position is better predicted. This is called teacher forcing. during training, it can only pay attention to words in previous earlier positions, thus, even though we have the entire correct sequence given to us with each training batch, we will hide the words in later positions through the mask to simulate real self attention (you cant pay self attention to words you havent said yet). In addition to suppressing/covering the pad tokens, the reply mask also covers sequence positions in the future. \n",
    "\n",
    "This is why the shape of the reply mask is **(batch_size, output_sequence_length, output_sequence_length)**, having 2 dimension named the same might be confusion, so i will rewrite this as \n",
    "\n",
    "**reply mask (batch_index, time_step, output_mask_position)**\n",
    "\n",
    "In the print statement below, I have shown you the mask `reply_mask[0,0,:]` indexed as `[0,0` for the first sample and first time step, `:` means all , it prints `True, False, . . . False` because at the first time step you can pay attention to the `<sos>` token only, not any future words in the \"correct reply\", at the next time step you can pay attention to both `<sos>` and also the first correct token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True]]]) torch.Size([2, 10, 10])\n",
      " ------------------------------------------------------ \n",
      "tensor([ True, False, False, False, False, False, False, False, False, False]) torch.Size([2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "listen_mask, reply_mask = create_masks(listen, reply_input, opt)\n",
    "print(listen_mask, reply_mask.shape)\n",
    "print(\" ------------------------------------------------------ \")\n",
    "print(reply_mask[0,0,:], reply_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the model (chloe's) output is **(batch_size, output_sequence_length, output_vocab_size)**\n",
    "For every reply in batch_size, chloe says output_sequence_length number of words, for each output position, `len(outfield.vocab)` numbers are given. `len(outfield.vocab)` is the number of words that are in chloe's output vocabulary. Yes, chloe has a separate vocab size for what she can hear and what she can say. suppose output_vocab_size = 100, then chloe only knows to output 100 different tokens. I say token, because this includes not only words, but also `<eos>`, `?`, `ummm`, `,` and other discreet symbols. \n",
    "\n",
    "Each of these numbers in `chloes_reply[0,-1,:]` represents one token, when I ran this cell the response included \n",
    "\n",
    "`'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3, 'you': 4, '?': 5,`\n",
    "\n",
    "`tensor([-1.7353, -0.5510, -1.4945,  7.2842,  2.6917,` \n",
    "\n",
    "and this makes alot of sense because `[0,-1,:]` indexes to the first example and the last output token. It includes all `:` the vocabulary. The tensor has in position 0, `-1.7353`, in position 3 it has `7.2842`, this was the highest number in the vector, it means that of all the tokens in the vocabulary, the word chloe things should come next is the `<eos>` token, 3 is the index for the end of sentence token  `'<eos>': 3,`, meaning that she is done talking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size = 42 , output_vocab_size = 65\n",
      " ------------------------------------------------------ \n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x12d1e0080>>, {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3, 'you': 4, '?': 5, ',': 6, 'i': 7, 'are': 8, 'joke': 9, 'say': 10, 'a': 11, 'bye': 12, 'cats': 13, 'do': 14, 'french': 15, 'hi': 16, 'how': 17, 'thank': 18, 'ttyl': 19, 'can': 20, 'for': 21, 'tell': 22, 'thanks': 23, 'alive': 24, 'and': 25, 'at': 26, 'beaucoup': 27, 'but': 28, 'dont': 29, 'either': 30, 'know': 31, 'laughing': 32, 'me': 33, 'meowci': 34, 'my': 35, 'ok': 36, 'some': 37, 'they': 38, 'to': 39, 'am': 40, 'biological': 41, 'bragging': 42, 'chloe': 43, 'conceding': 44, 'definition': 45, 'depends': 46, 'flash': 47, 'have': 48, 'just': 49, 'less': 50, 'math': 51, 'more': 52, 'of': 53, 'on': 54, 'robot': 55, 'sad': 56, 'science': 57, 'teach': 58, 'vicki': 59, 'viruses': 60, 'when': 61, 'will': 62, 'yes': 63, 'your': 64})\n",
      " ------------------------------------------------------ \n",
      "tensor([-1.7353, -0.5510, -1.4945,  7.2842,  2.6917, -0.6548, -1.6548, -0.2325,\n",
      "         0.8309, -1.2803,  4.0562,  1.2206,  0.1400, -2.4265,  3.8651,  7.2428,\n",
      "        -2.9040,  0.2920, -1.7389,  0.5245, -0.8791,  0.7126, -1.6746,  0.0768,\n",
      "        -0.4770, -2.5513,  2.2169, -0.4820,  2.6382, -0.2975,  3.3277,  2.3801,\n",
      "        -1.6595, -2.7537, -2.2505, -2.2646, -1.7907, -3.4850,  0.2276,  3.8092,\n",
      "        -0.4446, -0.2968, -0.6079, -4.4095, -0.3808, -3.8777, -2.9644, -2.9953,\n",
      "        -3.9381, -2.5299, -3.7081, -1.1889, -3.9064, -3.4756,  2.3643,  0.4258,\n",
      "        -1.0277, -1.3770, -4.0148, -0.9424, -2.5977, -0.6862, -0.1851, -2.5756,\n",
      "         1.5024], grad_fn=<SliceBackward>) torch.Size([2, 10, 65])\n"
     ]
    }
   ],
   "source": [
    "chloes_reply = chloe(listen, reply_input, listen_mask, reply_mask)\n",
    "\n",
    "print('input_vocab_size =', len(infield.vocab), ', output_vocab_size =', len(outfield.vocab))\n",
    "print(\" ------------------------------------------------------ \")\n",
    "print(outfield.vocab.stoi)\n",
    "print(\" ------------------------------------------------------ \")\n",
    "print(chloes_reply[0,-1,:], chloes_reply.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose the correct reply to listen = `bye`  `chloe` is reply = `see`  `ya` `<eos>`\n",
    "\n",
    "the batch we get gives is listen = `bye`  `chloe` and reply = `<sos>` `see`  `ya` `<eos>`\n",
    "\n",
    "we input `bye`  `chloe` into chloe, give her `<sos>` `see`  `ya`, plus the masks to prevent peaking into the future, and we compare whatever she replies against `see`  `ya` `<eos>`. I have been referring to `see`  `ya` `<eos>` as the \"correct response\", but this is also called the \"target\" abbreviated trg. In addition, what I have been referring to as \"listen\" is also called the \"source\" abbreviated src. \n",
    "\n",
    "## Loss functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x12d1e00f0>>, {'<unk>': 0, '<pad>': 1, '?': 2, 'you': 3, 'chloe': 4, 'i': 5, 'ok': 6, 'are': 7, 'hi': 8, 'a': 9, 'bye': 10, 'haha': 11, 'hello': 12, 'how': 13, 'later': 14, 'lol': 15, 'sure': 16, 'alive': 17, 'am': 18, 'any': 19, 'do': 20, 'dont': 21, 'dunno': 22, 'go': 23, 'gotta': 24, 'ill': 25, 'im': 26, 'joke': 27, 'know': 28, 'me': 29, 'more': 30, 'robot': 31, 'see': 32, 'talk': 33, 'tell': 34, 'think': 35, 'to': 36, 'true': 37, 'ttyl': 38, 'vicki': 39, 'what': 40, 'who': 41})\n",
      " ------------------------------------------------------ \n",
      "input_vocab_size = 42 , output_vocab_size = 65\n",
      " ------------------------------------------------------ \n",
      "tensor([[15, 11],\n",
      "        [11, 15]]) torch.Size([2, 2])\n",
      " ------------------------------------------------------ \n",
      "tensor([[ 2, 23, 21, 32, 26, 35,  9,  3],\n",
      "        [ 2, 23, 21, 32, 26, 35,  9,  3]]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, data_iterator, options, optimizer, scheduler):\n",
    "\n",
    "    if torch.cuda.is_available() and options.device == torch.device(\"cuda:0\"):\n",
    "        print(\"a GPU was detected, model will be trained on GPU\")\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        print(\"training on cpu\")\n",
    "\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    best_loss = 100\n",
    "    for epoch in range(options.epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(data_iterator): \n",
    "            src = batch.listen.transpose(0,1)\n",
    "            trg = batch.reply.transpose(0,1)\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, options)\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
    "                                         ys, ignore_index = options.trg_pad)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "        epoch_loss = total_loss/(num_batches(data_iterator)+1)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), options.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, epoch, epoch_loss))\n",
    "        total_loss = 0\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
