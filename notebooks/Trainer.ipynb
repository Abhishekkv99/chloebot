{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip3 install import-ipynb\n",
    "Requirement already satisfied: import-ipynb in /media/carson/New Volume/Chloe/chloebot/env36/lib/python3.6/site-packages (0.1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MoveData.ipynb\n",
      "importing Jupyter notebook from Encoder.ipynb\n",
      "importing Jupyter notebook from Elements.ipynb\n",
      "importing Jupyter notebook from Decoder.ipynb\n"
     ]
    }
   ],
   "source": [
    "import time, sys\n",
    "sys.path.append('/media/carson/New Volume/Chloe/chloebot/env36/lib/python3.6/site-packages')\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import Options, csv2datatools\n",
    "from MoveData import num_batches, nopeak_mask, create_masks\n",
    "from Encoder import Encoder\n",
    "from Decoder import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cdab6b55bc35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m opt = Options(batchsize=8, device=torch.device(\"cpu\"), epochs=25, \n\u001b[0m\u001b[1;32m      2\u001b[0m               lr=0.01, beam_width=3, max_len = 25, save_path = '../saved/weights/model_weights')\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson2datatools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Options' is not defined"
     ]
    }
   ],
   "source": [
    "opt = Options(batchsize=8, device=torch.device(\"cpu\"), epochs=25, \n",
    "              lr=0.01, beam_width=3, max_len = 25, save_path = '../saved/weights/model_weights')\n",
    "data_iter, infield, outfield, opt = json2datatools(opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim, n_layers, heads, dropout = 32, 3, 8, 0.01 \n",
    "chloe = Transformer(len(infield.vocab), len(outfield.vocab), emb_dim, n_layers, heads, dropout)\n",
    "chloe.load_state_dict(torch.load(opt.save_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, data_iterator, options, optimizer, scheduler):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    best_loss = 100\n",
    "    for epoch in range(options.epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(data_iterator): \n",
    "            src = batch.input_text.transpose(0,1)\n",
    "            trg = batch.output_text.transpose(0,1)\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, options)\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
    "                                         ys, ignore_index = options.trg_pad)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "        epoch_loss = total_loss/(num_batches(data_iterator)+1)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), options.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, epoch, epoch_loss))\n",
    "        total_loss = 0\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWithRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Cosine annealing with restarts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : torch.optim.Optimizer\n",
    "\n",
    "    T_max : int\n",
    "        The maximum number of iterations within the first cycle.\n",
    "\n",
    "    eta_min : float, optional (default: 0)\n",
    "        The minimum learning rate.\n",
    "\n",
    "    last_epoch : int, optional (default: -1)\n",
    "        The index of the last epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 T_max: int,\n",
    "                 eta_min: float = 0.,\n",
    "                 last_epoch: int = -1,\n",
    "                 factor: float = 1.) -> None:\n",
    "        # pylint: disable=invalid-name\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.factor = factor\n",
    "        self._last_restart: int = 0\n",
    "        self._cycle_counter: int = 0\n",
    "        self._cycle_factor: float = 1.\n",
    "        self._updated_cycle_len: int = T_max\n",
    "        self._initialized: bool = False\n",
    "        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Get updated learning rate.\"\"\"\n",
    "        # HACK: We need to check if this is the first time get_lr() was called, since\n",
    "        # we want to start with step = 0, but _LRScheduler calls get_lr with\n",
    "        # last_epoch + 1 when initialized.\n",
    "        if not self._initialized:\n",
    "            self._initialized = True\n",
    "            return self.base_lrs\n",
    "\n",
    "        step = self.last_epoch + 1\n",
    "        self._cycle_counter = step - self._last_restart\n",
    "\n",
    "        lrs = [\n",
    "            (\n",
    "                self.eta_min + ((lr - self.eta_min) / 2) *\n",
    "                (\n",
    "                    np.cos(\n",
    "                        np.pi *\n",
    "                        ((self._cycle_counter) % self._updated_cycle_len) /\n",
    "                        self._updated_cycle_len\n",
    "                    ) + 1\n",
    "                )\n",
    "            ) for lr in self.base_lrs\n",
    "        ]\n",
    "\n",
    "        if self._cycle_counter % self._updated_cycle_len == 0:\n",
    "            # Adjust the cycle length.\n",
    "            self._cycle_factor *= self.factor\n",
    "            self._cycle_counter = 0\n",
    "            self._updated_cycle_len = int(self._cycle_factor * self.T_max)\n",
    "            self._last_restart = step\n",
    "\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
