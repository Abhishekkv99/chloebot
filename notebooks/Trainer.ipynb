{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_batches(train):\n",
    "\n",
    "    for i, b in enumerate(train):\n",
    "        pass\n",
    "    \n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size, opt):\n",
    "    np_mask = np.triu(np.ones((1, size, size)),\n",
    "    k=1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "    if opt.device != -1:\n",
    "      np_mask = np_mask.cuda()\n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg, opt):\n",
    "    \n",
    "    src_mask = (src != opt.src_pad).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != opt.trg_pad).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, opt)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda()\n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, data_iterator, options, optimizer, scheduler):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    best_loss = 100\n",
    "    for epoch in range(options.epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(data_iterator): \n",
    "            src = batch.input_text.transpose(0,1)\n",
    "            trg = batch.output_text.transpose(0,1)\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, options)\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
    "                                         ys, ignore_index = options.trg_pad)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "        epoch_loss = total_loss/(num_batches(data_iterator)+1)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), options.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, epoch, epoch_loss))\n",
    "        total_loss = 0\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWithRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Cosine annealing with restarts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : torch.optim.Optimizer\n",
    "\n",
    "    T_max : int\n",
    "        The maximum number of iterations within the first cycle.\n",
    "\n",
    "    eta_min : float, optional (default: 0)\n",
    "        The minimum learning rate.\n",
    "\n",
    "    last_epoch : int, optional (default: -1)\n",
    "        The index of the last epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 T_max: int,\n",
    "                 eta_min: float = 0.,\n",
    "                 last_epoch: int = -1,\n",
    "                 factor: float = 1.) -> None:\n",
    "        # pylint: disable=invalid-name\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.factor = factor\n",
    "        self._last_restart: int = 0\n",
    "        self._cycle_counter: int = 0\n",
    "        self._cycle_factor: float = 1.\n",
    "        self._updated_cycle_len: int = T_max\n",
    "        self._initialized: bool = False\n",
    "        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Get updated learning rate.\"\"\"\n",
    "        # HACK: We need to check if this is the first time get_lr() was called, since\n",
    "        # we want to start with step = 0, but _LRScheduler calls get_lr with\n",
    "        # last_epoch + 1 when initialized.\n",
    "        if not self._initialized:\n",
    "            self._initialized = True\n",
    "            return self.base_lrs\n",
    "\n",
    "        step = self.last_epoch + 1\n",
    "        self._cycle_counter = step - self._last_restart\n",
    "\n",
    "        lrs = [\n",
    "            (\n",
    "                self.eta_min + ((lr - self.eta_min) / 2) *\n",
    "                (\n",
    "                    np.cos(\n",
    "                        np.pi *\n",
    "                        ((self._cycle_counter) % self._updated_cycle_len) /\n",
    "                        self._updated_cycle_len\n",
    "                    ) + 1\n",
    "                )\n",
    "            ) for lr in self.base_lrs\n",
    "        ]\n",
    "\n",
    "        if self._cycle_counter % self._updated_cycle_len == 0:\n",
    "            # Adjust the cycle length.\n",
    "            self._cycle_factor *= self.factor\n",
    "            self._cycle_counter = 0\n",
    "            self._updated_cycle_len = int(self._cycle_factor * self.T_max)\n",
    "            self._last_restart = step\n",
    "\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
