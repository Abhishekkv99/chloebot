{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import *\n",
    "from EncoderDecoder import *\n",
    "from Talk import *\n",
    "from Trainer import *\n",
    "from LearningDynamics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTransformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, \n",
    "                 heads, mem_slots, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = None\n",
    "        dim_k = emb_dim // heads\n",
    "        self.mem_slots = mem_slots\n",
    "        \n",
    "        self.encoder = Decoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "        \n",
    "        self.memory = torch.randn((1,self.mem_slots, emb_dim))\n",
    "        self.mem_mask = torch.ones(1,1,self.mem_slots) == 1\n",
    "        \n",
    "        self.mem_update = MultiHeadAttention(heads, emb_dim, dim_k, dropout)\n",
    "        self.normalizeMemory1 = Norm(emb_dim)\n",
    "        self.z_gate = nn.Linear(emb_dim*2, emb_dim)\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        \n",
    "    def update_memory(self):\n",
    "        mem_dialogue = torch.cat([self.memory, self.e_output, self.d_output], dim=-2) \n",
    "        new_memory, scores = self.mem_update(self.memory, mem_dialogue, mem_dialogue)\n",
    "        new_mem_norm = self.normalizeMemory1(new_memory + self.memory)\n",
    "        z_t = torch.sigmoid(self.z_gate(torch.cat([self.memory, new_mem_norm], dim=-1))) \n",
    "        self.memory = (1 - z_t)*self.memory + z_t*new_mem_norm\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        self.memory = self.repackage_hidden(self.memory)  \n",
    "        self.mem_mask = self.repackage_hidden(self.mem_mask)  \n",
    "        self.e_output = self.encoder(in_toks, in_mask, self.memory, self.mem_mask)\n",
    "        self.d_output = self.decoder(out_toks, out_mask, self.e_output, in_mask)\n",
    "        output = self.out(self.d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_model(input_str, model, opt, infield, outfield):\n",
    "    '''\n",
    "    input:\n",
    "        input_str is a string, it is what you want to say to the dialogue model\n",
    "        model is a encoder, decoder and a last layer linear transformation\n",
    "        opt is an options object with the maximum length of the output sequence opt.max_len\n",
    "        infield and outfield are the data.fields that store the vocabulary\n",
    "    output:\n",
    "        an output string response from the dialogue model\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    input_sequence = string2tensor(input_str, infield) # string to tensor \n",
    "    input_mask = (input_sequence != infield.vocab.stoi['<pad>']).unsqueeze(-2) #make input mask\n",
    "    model.e_output = model.encoder(input_sequence, input_mask, model.memory, model.mem_mask)\n",
    "    init_tok = outfield.vocab.stoi['<sos>'] # this is the integer for the start token\n",
    "    decoder_input = torch.LongTensor([[init_tok]]) # use start token to initiate the decoder\n",
    "    \n",
    "    for pos in range(opt.max_len):\n",
    "        decoder_input_mask = nopeak_mask(size=pos+1, opt=opt) # make target mask, pos+1 casue pos starts at 0\n",
    "        model.d_output = model.decoder(decoder_input, decoder_input_mask, model.e_output, input_mask)\n",
    "        out = model.out(model.d_output)\n",
    "        softout = F.softmax(out, dim=-1) \n",
    "\n",
    "        distr = Categorical(probs=softout)\n",
    "        action = distr.sample()[:,-1].unsqueeze(0) # sample from that distribution to get next token\n",
    "        decoder_input = torch.cat((decoder_input, action), dim=1) \n",
    "\n",
    "        if outfield.vocab.itos[action] == '<eos>':\n",
    "            de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0][1:-1]])\n",
    "            return de_str\n",
    "        \n",
    "    de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "    return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<sos> ! pillow pillow <pad> , <pad> , ? hi taco the taco how so how ! ? ! hi snuggles <pad> snuggles you bunny hello\n"
     ]
    }
   ],
   "source": [
    "opt = Options(batchsize=1, device = torch.device(\"cpu\"), epochs=20, lr=0.01, \n",
    "              max_len = 25, save_path = '../saved/weights/memory_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../saved/memory.json', opt=opt)\n",
    "\n",
    "emb_dim, n_layers, heads, mem_slots, dropout = 8, 3, 4, 2, 0.1\n",
    "\n",
    "chloe = MemoryTransformer(len(infield.vocab), len(outfield.vocab), emb_dim, n_layers, heads, mem_slots, dropout)\n",
    "\n",
    "print(talk_to_model(\"my name is bobo\", chloe, opt, infield, outfield))\n",
    "print(talk_to_model(\"what is my name?\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 0.459\n",
      "0m: epoch 1 loss = 0.312\n",
      "0m: epoch 4 loss = 0.273\n",
      "0m: epoch 8 loss = 0.273\n",
      "0m: epoch 15 loss = 0.272\n",
      "0m: epoch 24 loss = 0.253\n",
      "0m: epoch 25 loss = 0.190\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation_list = [\n",
    "    {\"listen\":\" \", \"reply\":\"so, how are you ?\"},\n",
    "    {\"listen\":\"my name is snuggles\", \"reply\":\"hello snuggles!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"snuggles the bunny\"},\n",
    "    {\"listen\":\"my name is bobo\", \"reply\":\"hi bobo!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"you are bobo\"},\n",
    "    {\"listen\":\"my name is taco\", \"reply\":\"sup taco !\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"taco\"},\n",
    "    {\"listen\":\"my name is fluffy\", \"reply\":\"hey fluffy!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"fluffy pillow\"},\n",
    "                    ]\n",
    "\n",
    "opt.lr = 0.01\n",
    "opt.epochs = 50 \n",
    "\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
    "\n",
    "sos_tok = torch.LongTensor([[outfield.vocab.stoi['<sos>']]]) \n",
    "eos_tok = torch.LongTensor([[outfield.vocab.stoi['<eos>']]]) \n",
    "\n",
    "chloe.train()\n",
    "start = time.time()\n",
    "best_loss = 100\n",
    "\n",
    "for epoch in range(opt.epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(conversation_list)):\n",
    "        listen_string = conversation_list[i][\"listen\"]\n",
    "        reply_string = conversation_list[i][\"reply\"]\n",
    "        listen_toks = string2tensor(listen_string, infield)\n",
    "        reply_toks = string2tensor(reply_string, outfield)\n",
    "        reply_start = torch.cat((sos_tok,reply_toks), dim=1)\n",
    "        reply_labels = torch.cat((reply_toks,eos_tok), dim=1).contiguous().view(-1)\n",
    "        \n",
    "        listen_mask, reply_mask = create_masks(listen_toks, reply_start, opt)\n",
    "        \n",
    "        logits = chloe(listen_toks, listen_mask, reply_start, reply_mask)\n",
    "        \n",
    "        chloe.update_memory()\n",
    "        \n",
    "        flat_logits = logits.view(-1, logits.size(-1))\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss=F.cross_entropy(flat_logits,reply_labels,ignore_index=opt.trg_pad)\n",
    "        batch_loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(chloe.parameters(), max_norm = 1.0) \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    epoch_loss = total_loss/len(conversation_list)\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(chloe.state_dict(), opt.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, epoch, epoch_loss))\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "print(\"finished\")\n",
    "load_subset_weights(chloe, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >     >  so , how are you ?\n",
      " >  my name is snuggles  >  hello snuggles !\n",
      " >  what is my name?  >  you are bobo\n",
      " >  my name is bobo  >  hi bobo !\n",
      " >  what is my name?  >  fluffy pillow\n",
      " >  my name is taco  >  sup taco !\n",
      " >  what is my name?  >  you are bobo\n",
      " >  my name is fluffy  >  hey fluffy !\n",
      " >  what is my name?  >  you are bobo\n",
      " >     >  so , how are you ?\n"
     ]
    }
   ],
   "source": [
    "load_subset_weights(chloe, opt) \n",
    "#scheduler = CosineWithRestarts(optimizer, T_max=len(conversation_list))\n",
    "\n",
    "chloe.eval()\n",
    "\n",
    "test_list = [\n",
    "    \" \", \n",
    "    \"my name is snuggles\", \n",
    "    \"what is my name?\", \n",
    "    \"my name is bobo\", \n",
    "    \"what is my name?\", \n",
    "    \"my name is taco\", \n",
    "    \"what is my name?\", \n",
    "    \"my name is fluffy\", \n",
    "    \"what is my name?\", \n",
    "    \" \",\n",
    "]\n",
    "\n",
    "for i in test_list:\n",
    "    print(\" > \", i, \" > \",  talk_to_model(i,chloe,opt,infield,outfield))\n",
    "    chloe.update_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You > my name is fluffy\n",
      "Chloe > hey fluffy !\n",
      "\n",
      "You > what is my name\n",
      "Chloe > you are bobo\n",
      "\n",
      "You > my name is fluffy\n",
      "Chloe > hey fluffy !\n",
      "\n",
      "You > what is my name\n",
      "Chloe > you are bobo\n",
      "\n",
      "You > what is my name?\n",
      "Chloe > taco\n",
      "\n",
      "You > whats my name\n",
      "Chloe > so , how are you ?\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-584-2338a2337335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0mtell_chloe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You > \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0mchloes_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtalk_to_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtell_chloe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchloe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mchloe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"bye chloe\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtell_chloe\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"bye ttyl\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchloes_reply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " while True:\n",
    "    tell_chloe = input(\"You > \")\n",
    "    chloes_reply = talk_to_model(tell_chloe, chloe, opt, infield, outfield)\n",
    "    chloe.update_memory()\n",
    "    if (\"bye chloe\" in tell_chloe or \"bye ttyl\" in chloes_reply):\n",
    "        print('Chloe > '+ chloes_reply + '\\n')\n",
    "        break\n",
    "    else:\n",
    "        print('Chloe > '+ chloes_reply + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to train the memory. How do we do this? we need to talk to the model and allow it to accumulate at least one cycle of conversation, then teach it to respond correctly given the previous listen-reply exchange"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
