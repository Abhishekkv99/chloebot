{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MoveData.ipynb\n"
     ]
    }
   ],
   "source": [
    "import time, sys, math, copy\n",
    "#sys.path.append('/path/to/env/lib/python3.6/site-packages')\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import Options, json2datatools, num_batches, nopeak_mask, create_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not using this notebook to learn, change the below variable `teaching` to False, if you are here to learn, change it to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into this lesson, lets remind ourselves where the components we are about to learn fit into the big picture. Recall the overall architecture of the Transformer. It is useful to image yourself as a piece of data, then imagine the journey you are about to go on. \n",
    "\n",
    "Imagine you emerge from the `data_iter` function and into the transformer, the first place you will go is the encoder. initialized using `self.encoder = Encoder(in_vocab_size, emb_dim, n_layers, heads, dropout)` and used in the form `e_output = self.encoder(src_seq, src_mask)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "    def forward(self, src_seq, trg_seq, src_mask, trg_mask):\n",
    "        e_output = self.encoder(src_seq, src_mask)\n",
    "        d_output = self.decoder(trg_seq, e_output, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component, or module, within the Encoder, is the Embedder. emb_dim is short for embedding_dimensions\n",
    "\n",
    "`self.embed = Embedder(vocab_size, embedding_dimensions)` \n",
    "\n",
    "`x = self.embed(source_sequence)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_layers, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = Embedder(vocab_size, emb_dim)\n",
    "        self.pe = PositionalEncoder(emb_dim, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(emb_dim, heads, dropout), n_layers)\n",
    "        self.norm = Norm(emb_dim)\n",
    "    def forward(self, src_seq, mask):\n",
    "        x = self.embed(src_seq)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x, mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is it Embedding? As we mentioned before, one ability, or limitation depending on how you look at it, of chloe is her fixed vocabulary, each word or symbol in her vocabulary is assigned an integer. For example the word hi is assigned 3, the word dog is 17, a word not in the vocabulary is 0. This integer is the `token` index. \n",
    "\n",
    "The neural nework sees every word as a vector. [A vector of 3 real numbers forms the coordinates in 3D space](https://youtu.be/fNk_zzaMoSs). We use several more dimensions than 3 in deep learning, if we use 512 dimensions, our `embedding_dimensions = 512`, this means that each word is a point in 512 dimensional space. The same concepts apply to 3D space in that the location of that word in 3D space tells you it's [meaning and meaning relative to other words](https://youtu.be/8rXD5-xhemo?t=1550).\n",
    "\n",
    "<img src=\"../saved/images/wordvectors.png\" height=400 width=400>\n",
    "\n",
    "In the image you see above, similar words are close to each other, not only that, the direction they are separated from eachother also carries meaning. In the image, there are 3 clusters of words and the separation between them has something to do with age/time/etc.  \n",
    "\n",
    "If you stack all the vectors on top of eachother row by row, you get a matrix. Remember how each word is represented by both a vector and an integer? well this integer is the index for a row in the matrix. The matrix is called the embedding matrix. you might say that we \"embed\" words into the matrix. Now for the example, run the cells below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the Embedder, I will show you it's two functions\n",
    "\n",
    "- storing an embedding matrix of word vectors \n",
    "- transforming a sequence of integers that represent token indices, into a sequence of vectors\n",
    "\n",
    "Lets start off by creating a toy embedding with only 2 tokens in it, these tokens will be represented in 4-dimensional space. After creating the embedding, I pass into the Embedder a sequence of token indices, the integers `[1,0,1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_sequence tensor([[1, 0, 1]]) torch.Size([1, 3])\n",
      "---------------------------------------------------\n",
      "Embedding Matrix Parameter containing:\n",
      "tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986,  0.4033,  0.8380]], requires_grad=True) torch.Size([2, 4])\n",
      "---------------------------------------------------\n",
      "sequence_of_vectors tensor([[[-1.0845, -1.3986,  0.4033,  0.8380],\n",
      "         [ 1.5410, -0.2934, -2.1788,  0.5684],\n",
      "         [-1.0845, -1.3986,  0.4033,  0.8380]]], grad_fn=<EmbeddingBackward>) torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "if teaching:  \n",
    "    torch.manual_seed(0)\n",
    "    embedding = Embedder(vocab_size=2, emb_dim=4)\n",
    "    source_sequence = torch.from_numpy(np.asarray([1,0,1])).unsqueeze(0)\n",
    "    print('source_sequence',source_sequence, source_sequence.shape)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"Embedding Matrix\", embedding.embed.weight, embedding.embed.weight.shape)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    sequence_of_vectors = embedding(source_sequence)\n",
    "    print('sequence_of_vectors',sequence_of_vectors, sequence_of_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, as I mentioned earlier, I am unhappy with chloe's limited vocabulary `{\"me\":0, \"give\":1}`. I want her to learn the word \"covfefe\". I will have to add a word to her dictionary `{\"covfefe\":2}` (not shown), then I will have to initialize a new word vector and add it, concatenate it, to chloe's embedding matrix `embedding.embed.weight`. In the cell below, I do just that. \n",
    "\n",
    "`concatenated_matrix` is the concatenation of the old matrix `embedding.embed.weight` with the `new_vector` appended to the bottom row. Now we can embed our\n",
    "\n",
    "sequence of integers **(batch_size, sequence_length)** \n",
    "\n",
    "into a vector sequence the phrase \"give me covfefe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_vector tensor([[0.0098, 0.0430, 0.0206, 0.0090]]) torch.Size([1, 4])\n",
      "---------------------------------------------------\n",
      "New Embedding Matrix Parameter containing:\n",
      "tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986,  0.4033,  0.8380],\n",
      "        [ 0.0098,  0.0430,  0.0206,  0.0090]], requires_grad=True) torch.Size([3, 4])\n",
      "---------------------------------------------------\n",
      "source_sequence tensor([[1, 0, 2]]) torch.Size([1, 3])\n",
      "---------------------------------------------------\n",
      "sequence_of_vectors tensor([[[-1.0845, -1.3986,  0.4033,  0.8380],\n",
      "         [ 1.5410, -0.2934, -2.1788,  0.5684],\n",
      "         [ 0.0098,  0.0430,  0.0206,  0.0090]]], grad_fn=<EmbeddingBackward>) torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "if teaching:  \n",
    "    np.random.seed(0)\n",
    "    new_vector = torch.from_numpy(np.random.uniform(-0.1,0.1,(1, 4)).astype(np.float32))\n",
    "    print('new_vector', new_vector, new_vector.shape)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    concatenated_matrix = torch.cat((embedding.embed.weight, new_vector),dim=0)\n",
    "    embedding.embed.weight=nn.Parameter(concatenated_matrix,requires_grad=True)\n",
    "    print(\"New Embedding Matrix\", embedding.embed.weight, embedding.embed.weight.shape)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    source_sequence = torch.from_numpy(np.asarray([1,0,2])).unsqueeze(0)\n",
    "    print('source_sequence',source_sequence, source_sequence.shape)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    sequence_of_vectors = embedding(source_sequence)\n",
    "    print('sequence_of_vectors',sequence_of_vectors, sequence_of_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "I find it useful to jeep track of the shape of my data as it goes on it's journey through the neurla network\n",
    "\n",
    "After the embedded, the shape is this vector sequence is **(batch_size, sequence_length, embedding_dimensions)**\n",
    "\n",
    "`[[-1.0845, -1.3986,  0.4033,  0.8380 ]` give\n",
    "\n",
    "` [ 1.5410, -0.2934, -2.1788,  0.5684 ]` me\n",
    "\n",
    "` [ 0.0098,  0.0430,  0.0206,  0.0090]]` covefefe\n",
    "  \n",
    " \n",
    " \n",
    " $$PE_{(pos,2i)} = \\sin(pos/1000^{2i/d_{model}})$$\n",
    " $$PE_{(pos,2i)} = \\cos(pos/1000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 200, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # create constant 'pe' matrix with values dependant on pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0) # add a batch dimention to your pe matrix \n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        #print('x.shape', x.shape) # (batch_size, input_seq_len, d_model)\n",
    "        #print('pe.shape', pe.shape) # (1, input_seq_len, d_model)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 4])\n",
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "if teaching: \n",
    "    positioner = PositionalEncoder(d_model=4, max_seq_len=100, dropout=0.0)\n",
    "    print(positioner.pe.shape)\n",
    "    print(positioner.pe[:,:3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_of_vectors tensor([[[ -8.6762,  -4.1888,   3.2268,  13.7042],\n",
      "         [ 18.2183,   4.6522, -17.4296,  11.5475],\n",
      "         [  6.4432,   7.3429,   0.1658,   7.0718]]], grad_fn=<AddBackward0>) torch.Size([1, 3, 4])\n",
      "---------------------------------------------------\n",
      "x.shape torch.Size([1, 3, 4])\n",
      "pe.shape torch.Size([1, 3, 4])\n",
      "tensor([[[-17.3524,  -7.3775,   6.4535,  28.4084],\n",
      "         [ 37.2780,  10.3044, -34.8591,  24.0949],\n",
      "         [ 13.7957,  15.6856,   0.3318,  15.1436]]], grad_fn=<AddBackward0>) torch.Size([1, 3, 4])\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if teaching: \n",
    "    print('sequence_of_vectors',sequence_of_vectors, sequence_of_vectors.shape)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    sequence_of_vectors = positioner(sequence_of_vectors)\n",
    "    print(sequence_of_vectors, sequence_of_vectors.shape)\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_of_vectors tensor([[[-2.1690, -1.7972,  0.8067,  2.6761],\n",
      "         [ 3.9235,  0.4131, -4.3575,  2.1369],\n",
      "         [ 0.9288,  1.0859,  0.0413,  1.0180]]], grad_fn=<AddBackward0>) torch.Size([1, 3, 4])\n",
      "---------------------------------------------------\n",
      "x.shape torch.Size([1, 3, 4])\n",
      "pe.shape torch.Size([1, 3, 4])\n",
      "tensor([[[-4.3381, -2.5944,  1.6134,  6.3521],\n",
      "         [ 8.6884,  1.8261, -8.7149,  5.2737],\n",
      "         [ 2.7669,  3.1716,  0.0828,  3.0359]]], grad_fn=<AddBackward0>) torch.Size([1, 3, 4])\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        \n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, emb_dim, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.k_dim = emb_dim // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(emb_dim, emb_dim)\n",
    "        self.v_linear = nn.Linear(emb_dim, emb_dim)\n",
    "        self.k_linear = nn.Linear(emb_dim, emb_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(emb_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.k_dim)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.k_dim)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.k_dim)\n",
    "        \n",
    "        # transpose to get dimensions bs * N * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.k_dim, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.emb_dim)\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim, ff_dim=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "    \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(emb_dim, ff_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(ff_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(emb_dim)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.attn = MultiHeadAttention(heads, emb_dim, dropout=dropout)\n",
    "        self.norm_2 = Norm(emb_dim)\n",
    "        self.ff = FeedForward(emb_dim, dropout=dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
