{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from LearningDynamics.ipynb\n"
     ]
    }
   ],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import *\n",
    "from EncoderDecoder import *\n",
    "from Talk import *\n",
    "from Trainer import *\n",
    "from LearningDynamics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_model(input_str, model, opt, infield, outfield):\n",
    "    '''\n",
    "    input:\n",
    "        input_str is a string, it is what you want to say to the dialogue model\n",
    "        model is a Transformer model with encoder, decoder and a last layer linear transformation\n",
    "        opt is an options object with the maximum length of the output sequence opt.max_len\n",
    "        infield and outfield are the data.fields that store the vocabulary\n",
    "    output:\n",
    "        an output string response from the dialogue model\n",
    "    Note: this version assumes we are evaluating the model on CPU \n",
    "    '''\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    input_sequence = string2tensor(input_str, infield) # string to tensor \n",
    "    input_mask = (input_sequence != infield.vocab.stoi['<pad>']).unsqueeze(-2) #make input mask\n",
    "    #encoding = model.encoder(input_sequence, input_mask, model.memory, model.mem_mask) # use the encoder rerepresent the input\n",
    "    encoding = model.encoder(input_sequence, input_mask)\n",
    "    init_tok = outfield.vocab.stoi['<sos>'] # this is the integer for the start token\n",
    "    decoder_input = torch.LongTensor([[init_tok]]) # use start token to initiate the decoder\n",
    "    \n",
    "    # continue obtaining the next decoder token until decoder outputs and end token or til max_len \n",
    "    for pos in range(opt.max_len):\n",
    "        decoder_input_mask = nopeak_mask(size=pos+1, opt=opt) # make target mask, pos+1 casue pos starts at 0\n",
    "        # the out vector contains the logits that are rebalanced by the softmax\n",
    "        out = model.out(model.decoder(decoder_input, decoder_input_mask, encoding, input_mask))\n",
    "        softout = F.softmax(out, dim=-1) \n",
    "        #softout is a categorical probability distribution over the output vocab\n",
    "        distr = Categorical(probs=softout)\n",
    "        action = distr.sample()[:,-1].unsqueeze(0) # sample from that distribution to get next token\n",
    "        # concatenate that token to our running list of output tokens \n",
    "        decoder_input = torch.cat((decoder_input, action), dim=1) \n",
    "        # if the model outputs an end of sentence token, it is done with this sentence\n",
    "        if outfield.vocab.itos[action] == '<eos>':\n",
    "            # [0] because we are assuming batch size of 1 \n",
    "            # [1:-1] excludes the start and end token from the output string \n",
    "            de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0][1:-1]])\n",
    "            return de_str\n",
    "        \n",
    "    de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "    return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTransformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, \n",
    "                 heads, mem_slots, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = None\n",
    "        dim_k = emb_dim // heads\n",
    "        self.mem_slots = mem_slots\n",
    "        \n",
    "        self.encoder = Encoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "\n",
    "    def update_memory(self):\n",
    "        mem_dialogue = torch.cat([self.memory, self.e_output, self.d_output], dim=-2) \n",
    "        new_memory, _ = self.MHDPA(self.memory, mem_dialogue, mem_dialogue)\n",
    "        new_mem_norm = self.NormalizeMemory(new_memory + self.memory)\n",
    "        z_t = torch.sigmoid(self.z_gate(self.memory))\n",
    "        self.memory = (1 - z_t)*self.memory + z_t*new_mem_norm\n",
    "        mem_mask = np.ones((1, 1, self.memory.size(-2))).astype('uint8')\n",
    "        self.mem_mask =  torch.from_numpy(mem_mask) == 1\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):  \n",
    "        self.in_encoded = self.encoder(in_toks, in_mask)\n",
    "        self.d_output = self.decoder(out_toks, out_mask, self.in_encoded, in_mask)\n",
    "        output = self.out(self.d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi fluffy\n"
     ]
    }
   ],
   "source": [
    "opt = Options(batchsize=1, device = torch.device(\"cpu\"), epochs=20, lr=0.005, \n",
    "              max_len = 25, save_path = '../saved/weights/memory_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../saved/memory.json', opt=opt)\n",
    "\n",
    "emb_dim, n_layers, heads, mem_slots, dropout = 32, 2, 8, 1, 0.01 \n",
    "chloe = MemoryTransformer(len(infield.vocab), len(outfield.vocab), \n",
    "                          emb_dim, n_layers, heads, mem_slots, dropout)\n",
    "\n",
    "#load_subset_weights(chloe, opt)\n",
    "print(talk_to_model(\"my name is fluffy\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 0.440\n",
      "0m: epoch 1 loss = 0.394\n",
      "0m: epoch 2 loss = 0.249\n",
      "0m: epoch 3 loss = 0.230\n",
      "0m: epoch 4 loss = 0.234\n",
      "0m: epoch 5 loss = 0.280\n",
      "0m: epoch 6 loss = 0.252\n",
      "0m: epoch 7 loss = 0.233\n",
      "0m: epoch 8 loss = 0.213\n",
      "0m: epoch 9 loss = 0.215\n",
      "0m: epoch 10 loss = 0.212\n",
      "0m: epoch 11 loss = 0.210\n",
      "0m: epoch 12 loss = 0.248\n",
      "0m: epoch 13 loss = 0.203\n",
      "0m: epoch 14 loss = 0.217\n",
      "0m: epoch 15 loss = 0.197\n",
      "0m: epoch 16 loss = 0.156\n",
      "0m: epoch 17 loss = 0.159\n",
      "0m: epoch 18 loss = 0.417\n",
      "0m: epoch 19 loss = 0.152\n",
      " >   my name is fluffy   >  hey fluffy !\n",
      " >   what is my name?   >  fluffy pillow\n",
      " >   my name is fluffy what is my name?  >  fluffy pillow\n",
      " >   my name is snuggles  >  hello snuggles !\n",
      " >   what is my name?   >  snuggles the bunny\n",
      " >   my name is snuggles what is my name?   >  snuggles the bunny\n",
      " >   my name is bobo   >  hi bobo !\n",
      " >   what is my name?   >  snuggles the bunny\n",
      " >   my name is bobo what is my name?   >  snuggles the bunny\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation_list = [\n",
    "{\"listen\":\"my name is fluffy\", \"reply\":\"hey fluffy!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"fluffy pillow\"},\n",
    "{\"listen\":\"my name is fluffy what is my name?\", \"reply\":\"fluffy pillow\"},\n",
    "{\"listen\":\"my name is snuggles\", \"reply\":\"hello snuggles!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"snuggles the bunny\"},\n",
    "{\"listen\":\"my name is snuggles what is my name?\", \"reply\":\"snuggles the bunny\"},\n",
    "{\"listen\":\"my name is bobo\", \"reply\":\"hi bobo!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"you are bobo\"},\n",
    "{\"listen\":\"my name is bobo what is my name?\", \"reply\":\"you are bobo\"},\n",
    "                    ]\n",
    "\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.6, patience=3)\n",
    "#scheduler = CosineWithRestarts(optimizer, T_max=len(conversation_list))\n",
    "\n",
    "sos_tok = torch.LongTensor([[outfield.vocab.stoi['<sos>']]]) \n",
    "eos_tok = torch.LongTensor([[outfield.vocab.stoi['<eos>']]]) \n",
    "\n",
    "chloe.train()\n",
    "start = time.time()\n",
    "best_loss = 100\n",
    "for epoch in range(opt.epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(conversation_list)):\n",
    "        listen_string = conversation_list[i][\"listen\"]\n",
    "        reply_string = conversation_list[i][\"reply\"]\n",
    "        listen_toks = string2tensor(listen_string, infield)\n",
    "        reply_toks = string2tensor(reply_string, outfield)\n",
    "        reply_start = torch.cat((sos_tok,reply_toks), dim=1)\n",
    "        reply_labels = torch.cat((reply_toks,eos_tok), dim=1).contiguous().view(-1)\n",
    "        \n",
    "        listen_mask, reply_mask = create_masks(listen_toks, reply_start, opt)\n",
    "        \n",
    "        logits = chloe(listen_toks, listen_mask, reply_start, reply_mask)\n",
    "        \n",
    "        #chloe.update_memory() # Update Memory\n",
    "        \n",
    "        flat_logits = logits.view(-1, logits.size(-1))\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = F.cross_entropy(flat_logits, reply_labels, ignore_index = opt.trg_pad)\n",
    "\n",
    "        batch_loss.backward() #batch_loss.backward(retain_graph=True) #\n",
    "        torch.nn.utils.clip_grad_norm_(chloe.parameters(), max_norm = 1.0) \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    epoch_loss = total_loss/len(conversation_list)\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(chloe.state_dict(), opt.save_path)\n",
    "    print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, \n",
    "                                        epoch, epoch_loss))\n",
    "    total_loss = 0\n",
    "\n",
    "chloe.eval()\n",
    "\n",
    "test_list = [\n",
    "    \" my name is fluffy \",\n",
    "    \" what is my name? \",\n",
    "    \" my name is fluffy what is my name?\",\n",
    "    \" my name is snuggles\",\n",
    "    \" what is my name? \",\n",
    "    \" my name is snuggles what is my name? \",\n",
    "    \" my name is bobo \",\n",
    "    \" what is my name? \",\n",
    "    \" my name is bobo what is my name? \"\n",
    "]\n",
    "\n",
    "for i in test_list:\n",
    "    print(\" > \", i, \" > \",  talk_to_model(i,chloe,opt,infield,outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n",
      "0m: epoch 0 loss = 0.263\n",
      "0m: epoch 1 loss = 0.136\n",
      "0m: epoch 2 loss = 0.173\n",
      "0m: epoch 3 loss = 0.277\n",
      "0m: epoch 4 loss = 0.193\n",
      "0m: epoch 5 loss = 0.148\n",
      "0m: epoch 6 loss = 0.152\n",
      "0m: epoch 7 loss = 0.113\n",
      "0m: epoch 8 loss = 0.131\n",
      "0m: epoch 9 loss = 0.080\n",
      "0m: epoch 10 loss = 0.064\n",
      "0m: epoch 11 loss = 0.167\n",
      "0m: epoch 12 loss = 0.122\n",
      "0m: epoch 13 loss = 0.100\n",
      "0m: epoch 14 loss = 0.159\n",
      "0m: epoch 15 loss = 0.081\n",
      "0m: epoch 16 loss = 0.087\n"
     ]
    }
   ],
   "source": [
    "def trainer(model, data_iterator, options, optimizer, scheduler):\n",
    "\n",
    "    if torch.cuda.is_available() and options.device == torch.device(\"cuda:0\"):\n",
    "        print(\"a GPU was detected, model will be trained on GPU\")\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        print(\"training on cpu\")\n",
    "\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    best_loss = 100\n",
    "    for epoch in range(options.epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(data_iterator): \n",
    "            src = batch.listen.transpose(0,1)\n",
    "            trg = batch.reply.transpose(0,1)\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, options)\n",
    "            preds = model(src, src_mask, trg_input, trg_mask)\n",
    "            \n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
    "                                         ys, ignore_index = options.trg_pad)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "        epoch_loss = total_loss/(num_batches(data_iterator)+1)\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), options.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, epoch, epoch_loss))\n",
    "        total_loss = 0\n",
    "\n",
    "    return model\n",
    "#load_subset_weights(chloe, opt)\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=3)\n",
    "\n",
    "chloe = trainer(chloe, data_iter, opt, optimizer, scheduler)\n",
    "print(talk_to_chloe(\"my name is snuggles\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory() # Update Memory \n",
    "print(talk_to_chloe(\"what is my name?\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 0.446\n",
      "0m: epoch 1 loss = 0.719\n",
      "0m: epoch 2 loss = 0.354\n",
      "0m: epoch 3 loss = 0.475\n",
      "0m: epoch 4 loss = 0.280\n",
      "0m: epoch 5 loss = 0.335\n",
      "0m: epoch 6 loss = 0.250\n",
      "0m: epoch 7 loss = 0.215\n",
      "0m: epoch 8 loss = 0.194\n",
      "0m: epoch 9 loss = 0.190\n",
      "0m: epoch 10 loss = 0.190\n",
      "0m: epoch 11 loss = 0.193\n",
      "0m: epoch 12 loss = 0.192\n",
      "0m: epoch 13 loss = 0.193\n",
      "0m: epoch 14 loss = 0.191\n",
      "0m: epoch 15 loss = 0.191\n",
      "0m: epoch 16 loss = 0.193\n",
      "0m: epoch 17 loss = 0.191\n",
      "0m: epoch 18 loss = 0.191\n",
      "0m: epoch 19 loss = 0.190\n",
      "> my name is fluffy > hello fluffy !\n",
      "> what is my name? > hello bobo !\n",
      "> my name is snuggles > hello snuggles !\n",
      "> what is my name? > hello bobo !\n",
      "> my name is bobo > its bobo silly\n",
      "> what is my name? > its bobo silly\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to train the memory. How do we do this? we need to talk to the model and allow it to accumulate at least one cycle of conversation, then teach it to respond correctly given the previous listen-reply exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meowci beaucoup !\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank am hi\n",
      "<unk> meowci <unk>\n",
      "thank meowci hi\n",
      "<unk> meowci <unk>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
