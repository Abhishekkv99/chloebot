{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MoveData.ipynb\n",
      "importing Jupyter notebook from EncoderDecoder.ipynb\n",
      "importing Jupyter notebook from Elements.ipynb\n",
      "importing Jupyter notebook from Talk.ipynb\n",
      "importing Jupyter notebook from Trainer.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/carson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import *\n",
    "from EncoderDecoder import *\n",
    "from Talk import *\n",
    "from Trainer import *\n",
    "from Talk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_chloe(input_str, model, opt, infield, outfield):\n",
    "    '''\n",
    "    input:\n",
    "        input_str is a string, it is what you want to say to the dialogue model\n",
    "        model is a Transformer model with encoder, decoder and a last layer linear transformation\n",
    "        opt is an options object with the maximum length of the output sequence opt.max_len\n",
    "        infield and outfield are the data.fields that store the vocabulary\n",
    "    output:\n",
    "        an output string response from the dialogue model\n",
    "    \n",
    "    Note: this version assumes we are evaluating the model on CPU \n",
    "    '''\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    input_sequence = string2tensor(input_str, infield) # string to tensor \n",
    "    input_mask = (input_sequence != infield.vocab.stoi['<pad>']).unsqueeze(-2) #make input mask\n",
    "    #encoding = model.encoder(input_sequence, input_mask, model.memory, model.mem_mask) # use the encoder rerepresent the input\n",
    "    encoding = model.encoder(input_sequence, input_mask)\n",
    "    init_tok = outfield.vocab.stoi['<sos>'] # this is the integer for the start token\n",
    "    decoder_input = torch.LongTensor([[init_tok]]) # use start token to initiate the decoder\n",
    "    \n",
    "    # continue obtaining the next decoder token until decoder outputs and end token or til max_len \n",
    "    for pos in range(opt.max_len):\n",
    "        decoder_input_mask = nopeak_mask(size=pos+1, opt=opt) # make target mask, pos+1 casue pos starts at 0\n",
    "        # the out vector contains the logits that are rebalanced by the softmax\n",
    "        out = model.out(model.decoder(decoder_input, decoder_input_mask, encoding, input_mask))\n",
    "        softout = F.softmax(out, dim=-1) \n",
    "        #softout is a categorical probability distribution over the output vocab\n",
    "        distr = Categorical(probs=softout)\n",
    "        action = distr.sample()[:,-1].unsqueeze(0) # sample from that distribution to get next token\n",
    "        # concatenate that token to our running list of output tokens \n",
    "        decoder_input = torch.cat((decoder_input, action), dim=1) \n",
    "        # if the model outputs an end of sentence token, it is done with this sentence\n",
    "        if outfield.vocab.itos[action] == '<eos>':\n",
    "            # [0] because we are assuming batch size of 1 \n",
    "            # [1:-1] excludes the start and end token from the output string \n",
    "            de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0][1:-1]])\n",
    "            return de_str\n",
    "        \n",
    "    de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "    return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTransformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, \n",
    "                 heads, mem_slots, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = None\n",
    "        dim_k = emb_dim // heads\n",
    "        self.mem_slots = mem_slots\n",
    "        \n",
    "        self.encoder = Encoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        #self.encoder = Decoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        self.memory = torch.randn((self.mem_slots, emb_dim))\n",
    "        mem_mask = np.ones((1,self.mem_slots)).astype('uint8')\n",
    "        self.mem_mask =  torch.from_numpy(mem_mask) == 1\n",
    "        \n",
    "        self.mem_update = MultiHeadAttention(heads, emb_dim, dim_k, dropout)\n",
    "        self.z_gate = nn.Linear(emb_dim, emb_dim)\n",
    "        self.NormalizeMemory = Norm(emb_dim)\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "        # needed for truncated BPTT, called at every batch forward pass\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        \n",
    "    def batch_memory(self, in_toks):\n",
    "        self.batch_size = in_toks.size(0)\n",
    "        self.memory = torch.stack([self.memory for _ in range(self.batch_size)])\n",
    "        self.mem_mask = torch.stack([self.mem_mask for _ in range(self.batch_size)])\n",
    "        print(\"setting batch size to \", self.batch_size)\n",
    "        \n",
    "    def update_memory(self):\n",
    "        mem_dialogue = torch.cat([self.e_output, self.d_output], dim=-2) \n",
    "        #mem_dialogue = torch.cat([self.memory, self.d_output], dim=-2)\n",
    "        new_memory, scores = self.mem_update(self.memory, mem_dialogue, mem_dialogue)\n",
    "        new_mem_norm = self.NormalizeMemory(new_memory + self.memory)\n",
    "        z_t = torch.sigmoid(self.z_gate(self.memory)) # (batch size, memory slots, memory size)\n",
    "        self.memory = (1 - z_t)*self.memory + z_t*new_mem_norm\n",
    "        #self.memory = torch.cat([self.e_output, self.d_output], dim=-2) \n",
    "        #mem_mask = np.ones((1, 1, self.memory.size(-2))).astype('uint8')\n",
    "        #self.mem_mask =  torch.from_numpy(mem_mask) == 1\n",
    "\n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        self.memory = self.repackage_hidden(self.memory)\n",
    "        if self.batch_size == None: self.batch_memory(in_toks)   \n",
    "        self.e_output = self.encoder(in_toks, in_mask)\n",
    "        #self.e_output = self.encoder(in_toks, in_mask, self.memory, self.mem_mask)\n",
    "        mem_en_vecs = torch.cat([self.e_output, self.memory], dim=-2) \n",
    "        mem_en_mask = torch.from_numpy(np.ones((self.batch_size,1,mem_en_vecs.size(-2))))==1\n",
    "        self.d_output = self.decoder(out_toks, out_mask, mem_en_vecs, mem_en_mask)\n",
    "        #self.d_output = self.decoder(out_toks, out_mask, self.e_output, in_mask)\n",
    "        output = self.out(self.d_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt = Options(batchsize=1, device = torch.device(\"cpu\"), epochs=50, lr=0.01, \n",
    "              max_len = 25, save_path = '../saved/weights/memory2_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../saved/memory.json', opt=opt)\n",
    "\n",
    "emb_dim, n_layers, heads, mem_slots, dropout = 32, 2, 4, 1, 0.1 \n",
    "chloe = MemoryTransformer(len(infield.vocab), len(outfield.vocab), emb_dim, \n",
    "                          n_layers, heads, mem_slots, dropout)\n",
    "\n",
    "#load_subset_weights(chloe, opt)\n",
    "print(talk_to_chloe(\"my name is fluffy\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 0.468\n",
      "0m: epoch 1 loss = 0.412\n",
      "0m: epoch 2 loss = 0.401\n",
      "0m: epoch 3 loss = 0.330\n",
      "0m: epoch 4 loss = 0.316\n",
      "0m: epoch 5 loss = 0.321\n",
      "0m: epoch 6 loss = 0.596\n",
      "0m: epoch 7 loss = 0.298\n",
      "0m: epoch 8 loss = 0.560\n",
      "0m: epoch 9 loss = 0.346\n",
      "0m: epoch 10 loss = 0.713\n",
      "0m: epoch 11 loss = 0.697\n",
      "0m: epoch 12 loss = 0.579\n",
      "0m: epoch 13 loss = 0.343\n",
      "0m: epoch 14 loss = 0.310\n",
      "0m: epoch 15 loss = 0.339\n",
      "0m: epoch 16 loss = 0.417\n",
      "0m: epoch 17 loss = 0.327\n",
      "0m: epoch 18 loss = 0.319\n",
      "0m: epoch 19 loss = 0.305\n",
      "0m: epoch 20 loss = 0.472\n",
      "0m: epoch 21 loss = 0.379\n",
      "0m: epoch 22 loss = 0.321\n",
      "0m: epoch 23 loss = 0.305\n",
      "0m: epoch 24 loss = 0.340\n",
      "0m: epoch 25 loss = 0.385\n",
      "0m: epoch 26 loss = 0.336\n",
      "0m: epoch 27 loss = 0.340\n",
      "0m: epoch 28 loss = 0.393\n",
      "0m: epoch 29 loss = 0.301\n",
      "0m: epoch 30 loss = 0.315\n",
      "0m: epoch 31 loss = 0.307\n",
      "0m: epoch 32 loss = 0.343\n",
      "0m: epoch 33 loss = 0.333\n",
      "0m: epoch 34 loss = 0.335\n",
      "0m: epoch 35 loss = 0.298\n",
      "0m: epoch 36 loss = 0.322\n",
      "0m: epoch 37 loss = 0.322\n",
      "0m: epoch 38 loss = 0.341\n",
      "0m: epoch 39 loss = 0.314\n",
      "0m: epoch 40 loss = 0.307\n",
      "0m: epoch 41 loss = 0.352\n",
      "0m: epoch 42 loss = 0.261\n",
      "0m: epoch 43 loss = 0.288\n",
      "0m: epoch 44 loss = 0.346\n",
      "0m: epoch 45 loss = 0.329\n",
      "0m: epoch 46 loss = 0.286\n",
      "0m: epoch 47 loss = 0.295\n",
      "0m: epoch 48 loss = 0.342\n",
      "0m: epoch 49 loss = 0.332\n",
      "> my name is fluffy > hello fluffy !\n",
      "> what is my name? > fluffy fluffy fluffy pillows\n",
      "> my name is snuggles > hello bobo !\n",
      "> what is my name? > snuggles like snuggle bunny\n",
      "> my name is bobo > hello bobo !\n",
      "> what is my name? > snuggles like snuggle bunny\n",
      "> my name is nacho > hello nacho !\n",
      "> what is my name? > fluffy pillows\n"
     ]
    }
   ],
   "source": [
    "conversation_list = [\n",
    "    {\"listen\":\"my name is fluffy\", \"reply\":\"hello fluffy!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"fluffy fluffy pillows\"},\n",
    "    {\"listen\":\"my name is snuggles\", \"reply\":\"hello snuggles!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"snuggles like snuggle bunny\"},\n",
    "    {\"listen\":\"my name is bobo\", \"reply\":\"hello bobo!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"bobo boe tie\"},\n",
    "    {\"listen\":\"my name is nacho\", \"reply\":\"hello nacho!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"nacho nacho cheese\"},\n",
    "                    ]\n",
    "\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',factor=0.9,patience=2)\n",
    "\n",
    "sos_tok = torch.LongTensor([[outfield.vocab.stoi['<sos>']]]) \n",
    "eos_tok = torch.LongTensor([[outfield.vocab.stoi['<eos>']]]) \n",
    "\n",
    "chloe.train()\n",
    "start = time.time()\n",
    "best_loss = 100\n",
    "for epoch in range(opt.epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(conversation_list)):\n",
    "        listen_string = conversation_list[i][\"listen\"]\n",
    "        reply_string = conversation_list[i][\"reply\"]\n",
    "        listen_toks = string2tensor(listen_string, infield)\n",
    "        reply_toks = string2tensor(reply_string, outfield)\n",
    "        reply_start = torch.cat((sos_tok,reply_toks), dim=1)\n",
    "        reply_labels = torch.cat((reply_toks,eos_tok), dim=1).contiguous().view(-1)\n",
    "        \n",
    "        listen_mask, reply_mask = create_masks(listen_toks, reply_start, opt)\n",
    "        \n",
    "        logits = chloe(listen_toks, listen_mask, reply_start, reply_mask)\n",
    "        \n",
    "        chloe.update_memory() # Update Memory\n",
    "        \n",
    "        flat_logits = logits.view(-1, logits.size(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = F.cross_entropy(flat_logits, reply_labels, ignore_index = opt.trg_pad)\n",
    "        \n",
    "        #if i % 2 == 1:\n",
    "        batch_loss.backward() #batch_loss.backward(retain_graph=True) #\n",
    "        #torch.nn.utils.clip_grad_norm_(chloe.parameters(), max_norm = 1.0) \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    epoch_loss = total_loss/len(conversation_list)\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(chloe.state_dict(), opt.save_path)\n",
    "        \n",
    "    print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, \n",
    "                                        epoch, epoch_loss))\n",
    "    total_loss = 0\n",
    "\n",
    "chloe.eval()\n",
    "print(\"> my name is fluffy >\", talk_to_chloe(\"my name is fluffy\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> what is my name? >\", talk_to_chloe(\"what is my name?\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> my name is snuggles >\", talk_to_chloe(\"my name is snuggles\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> what is my name? >\", talk_to_chloe(\"what is my name?\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> my name is bobo >\", talk_to_chloe(\"my name is bobo\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> what is my name? >\", talk_to_chloe(\"what is my name?\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> my name is nacho >\", talk_to_chloe(\"my name is nacho\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> what is my name? >\", talk_to_chloe(\"what is my name?\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n",
      "0m: epoch 0 loss = 0.263\n",
      "0m: epoch 1 loss = 0.136\n",
      "0m: epoch 2 loss = 0.173\n",
      "0m: epoch 3 loss = 0.277\n",
      "0m: epoch 4 loss = 0.193\n",
      "0m: epoch 5 loss = 0.148\n",
      "0m: epoch 6 loss = 0.152\n",
      "0m: epoch 7 loss = 0.113\n",
      "0m: epoch 8 loss = 0.131\n",
      "0m: epoch 9 loss = 0.080\n",
      "0m: epoch 10 loss = 0.064\n",
      "0m: epoch 11 loss = 0.167\n",
      "0m: epoch 12 loss = 0.122\n",
      "0m: epoch 13 loss = 0.100\n",
      "0m: epoch 14 loss = 0.159\n",
      "0m: epoch 15 loss = 0.081\n",
      "0m: epoch 16 loss = 0.087\n"
     ]
    }
   ],
   "source": [
    "def trainer(model, data_iterator, options, optimizer, scheduler):\n",
    "\n",
    "    if torch.cuda.is_available() and options.device == torch.device(\"cuda:0\"):\n",
    "        print(\"a GPU was detected, model will be trained on GPU\")\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        print(\"training on cpu\")\n",
    "\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    best_loss = 100\n",
    "    for epoch in range(options.epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(data_iterator): \n",
    "            src = batch.listen.transpose(0,1)\n",
    "            trg = batch.reply.transpose(0,1)\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, options)\n",
    "            preds = model(src, src_mask, trg_input, trg_mask)\n",
    "            \n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
    "                                         ys, ignore_index = options.trg_pad)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "        epoch_loss = total_loss/(num_batches(data_iterator)+1)\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), options.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, epoch, epoch_loss))\n",
    "        total_loss = 0\n",
    "\n",
    "    return model\n",
    "#load_subset_weights(chloe, opt)\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=3)\n",
    "\n",
    "chloe = trainer(chloe, data_iter, opt, optimizer, scheduler)\n",
    "print(talk_to_chloe(\"my name is snuggles\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory() # Update Memory \n",
    "print(talk_to_chloe(\"what is my name?\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 0.446\n",
      "0m: epoch 1 loss = 0.719\n",
      "0m: epoch 2 loss = 0.354\n",
      "0m: epoch 3 loss = 0.475\n",
      "0m: epoch 4 loss = 0.280\n",
      "0m: epoch 5 loss = 0.335\n",
      "0m: epoch 6 loss = 0.250\n",
      "0m: epoch 7 loss = 0.215\n",
      "0m: epoch 8 loss = 0.194\n",
      "0m: epoch 9 loss = 0.190\n",
      "0m: epoch 10 loss = 0.190\n",
      "0m: epoch 11 loss = 0.193\n",
      "0m: epoch 12 loss = 0.192\n",
      "0m: epoch 13 loss = 0.193\n",
      "0m: epoch 14 loss = 0.191\n",
      "0m: epoch 15 loss = 0.191\n",
      "0m: epoch 16 loss = 0.193\n",
      "0m: epoch 17 loss = 0.191\n",
      "0m: epoch 18 loss = 0.191\n",
      "0m: epoch 19 loss = 0.190\n",
      "> my name is fluffy > hello fluffy !\n",
      "> what is my name? > hello bobo !\n",
      "> my name is snuggles > hello snuggles !\n",
      "> what is my name? > hello bobo !\n",
      "> my name is bobo > its bobo silly\n",
      "> what is my name? > its bobo silly\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to train the memory. How do we do this? we need to talk to the model and allow it to accumulate at least one cycle of conversation, then teach it to respond correctly given the previous listen-reply exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meowci beaucoup !\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank am hi\n",
      "<unk> meowci <unk>\n",
      "thank meowci hi\n",
      "<unk> meowci <unk>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
