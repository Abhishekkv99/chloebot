{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MoveData.ipynb\n",
      "importing Jupyter notebook from Elements.ipynb\n",
      "importing Jupyter notebook from Talk.ipynb\n",
      "importing Jupyter notebook from EncoderDecoder.ipynb\n",
      "importing Jupyter notebook from Trainer.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/carson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import *\n",
    "from Elements import MultiHeadAttention, Norm, FeedForward\n",
    "from Talk import *\n",
    "from Trainer import *\n",
    "from Talk import talk_to_chloe, get_synonym, string2tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MemoryTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching = True\n",
    "\n",
    "class MemoryTransformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, num_heads, mem_slots, dropout):\n",
    "        \n",
    "        super(MemoryTransformer, self).__init__() \n",
    "        \n",
    "        self.mem_slots = mem_slots\n",
    "        self.mem_size = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_k = self.mem_size // self.num_heads\n",
    "        \n",
    "        self.encoder = Encoder(in_vocab_size, emb_dim, n_layers, num_heads, dropout)\n",
    "        self.rmc = RelMemCore(mem_slots, mem_size=emb_dim, num_heads=num_heads)\n",
    "        \n",
    "        self.current_memory = self.rmc.initial_memory(batch_size=1)\n",
    "        \n",
    "        self.mem_encoder = MultiHeadAttention(num_heads,self.mem_size,self.dim_k,dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, num_heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "             \n",
    "    def forward(self, src_seq, trg_seq, src_mask, trg_mask):\n",
    "        e_output = self.encoder(src_seq, src_mask)\n",
    "        m_output, m_scores = self.mem_encoder(e_output,self.current_memory,self.current_memory)\n",
    "        d_output = self.decoder(trg_seq, m_output, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meowci beaucoup !\n"
     ]
    }
   ],
   "source": [
    "if teaching:\n",
    "    opt = Options(batchsize=2, device = torch.device(\"cpu\"), epochs=25, lr=0.01, \n",
    "                  beam_width=3, max_len = 25, save_path = '../saved/weights/model_weights')\n",
    "\n",
    "    data_iter, infield, outfield, opt = json2datatools(path='../saved/pairs.json', opt=opt)\n",
    "\n",
    "    emb_dim, n_layers, num_heads, mem_slots, dropout = 32, 3, 8, 4, 0.01 \n",
    "    chloe = MemoryTransformer(len(infield.vocab), len(outfield.vocab), \n",
    "                              emb_dim, n_layers, num_heads, mem_slots, dropout)\n",
    "    \n",
    "    load_subset_weights(chloe, opt)\n",
    "    print(talk_to_chloe(\"how?\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk2model_MemRL(input_str, model, opt, infield, outfield):\n",
    "    '''\n",
    "    input:\n",
    "        input_str is a string, it is what you want to say to the dialogue model\n",
    "        model is a Transformer model with encoder, decoder and a last layer linear transformation\n",
    "        opt is an options object with the maximum length of the output sequence opt.max_len\n",
    "        infield and outfield are the data.fields that store the vocabulary\n",
    "    output:\n",
    "        an output string response from the dialogue model\n",
    "    \n",
    "    Note: this version assumes we are evaluating the model on CPU \n",
    "    '''\n",
    "    input_sequence = string2tensor(input_str, infield) # string to tensor \n",
    "    input_mask = (input_sequence != infield.vocab.stoi['<pad>']).unsqueeze(-2) #make input mask\n",
    "    encoding = model.encoder(input_sequence, input_mask) # use the encoder rerepresent the input\n",
    "    encoding, m_scores = model.mem_encoder(encoding,model.current_memory,model.current_memory)\n",
    "    init_tok = outfield.vocab.stoi['<sos>'] # this is the integer for the start token\n",
    "    decoder_input = torch.LongTensor([[init_tok]]) # use start token to initiate the decoder\n",
    "    logprobs = torch.Tensor([[]])\n",
    "    \n",
    "    # continue obtaining the next decoder token until decoder outputs and end token or til max_len\n",
    "    for pos in range(opt.max_len):\n",
    "        decoder_input_mask = nopeak_mask(size=pos+1, opt=opt) # make target mask, pos+1 casue pos starts at 0\n",
    "        # the out vector contains the logits that are rebalanced by the softmax\n",
    "        out = model.out(model.decoder(decoder_input, encoding, input_mask, decoder_input_mask))\n",
    "        #softout is a categorical probability distribution over the output vocab\n",
    "        softout = F.softmax(out, dim=-1)\n",
    "        distr = Categorical(probs=softout)\n",
    "        action = distr.sample()[:,-1].unsqueeze(0)\n",
    "        logprob = -distr.log_prob(action)[:,-1].unsqueeze(0)\n",
    "        # concatenate that token to our running list of output tokens \n",
    "        decoder_input = torch.cat((decoder_input, action), dim=1)\n",
    "        logprobs = torch.cat((logprobs, logprob), dim=1)\n",
    "        # if the model outputs an end of sentence token, it is done with this sentence\n",
    "        if outfield.vocab.itos[action] == '<eos>':\n",
    "            # [0] because we are assuming batch size of 1 \n",
    "            # [1:-1] excludes the start and end token from the output string \n",
    "            de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "            decoder_input_mask = nopeak_mask(size=pos+2, opt=opt) \n",
    "            memory_sequence = model.decoder(decoder_input, encoding, input_mask, decoder_input_mask)\n",
    "            memory_vector = memory_sequence[:,-1,:] # get the last vector as a summary\n",
    "            model.current_memory = model.rmc.update_memory(memory_vector, model.current_memory)\n",
    "            return decoder_input, de_str, logprobs\n",
    "\n",
    "    remember_token = torch.LongTensor([[outfield.vocab.stoi['<eos>']]])\n",
    "    remember_sequence = torch.cat((decoder_input, remember_token), dim=1)\n",
    "    decoder_input_mask = nopeak_mask(size=pos+3, opt=opt) \n",
    "    memory_sequence = model.decoder(remember_sequence, encoding, input_mask, decoder_input_mask)\n",
    "    memory_vector = memory_sequence[:,-1,:]  # get the last vector as a summary\n",
    "    model.current_memory = model.rmc.update_memory(memory_vector, model.current_memory)\n",
    "    de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "    return decoder_input, de_str, logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> meowci beaucoup ! <eos>\n"
     ]
    }
   ],
   "source": [
    "decoder_input, de_str, logprobs = talk2model_MemRL(\"how ?\", chloe, opt, infield, outfield)\n",
    "print(de_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to train the memory. How do we do this? we need to talk to the model and allow it to accumulate at least one cycle of conversation, then teach it to respond correctly given the previous listen-reply exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_list = [\n",
    "    {\"listen\":\"my name is fluffy\", \"reply\":\"hello fluffy!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"its fluffy silly\"},\n",
    "    {\"listen\":\"my name is snuggles\", \"reply\":\"hello snuggles!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"its snuggles silly\"},\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convo_trainer(conversation_list, model, options):\n",
    "\n",
    "    optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "\n",
    "    sos_tok = torch.LongTensor([[outfield.vocab.stoi['<sos>']]]) \n",
    "    eos_tok = torch.LongTensor([[outfield.vocab.stoi['<eos>']]]) \n",
    "\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    best_loss = 100\n",
    "    for epoch in range(options.epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(conversation_list)):\n",
    "            listen_sequence = string2tensor(conversation_list[i][\"listen\"], infield)\n",
    "            reply_sequence = string2tensor(conversation_list[i][\"reply\"], infield)\n",
    "            decoder_input = torch.cat((sos_tok,reply_sequence,eos_tok), dim=1)\n",
    "            decoder_target = torch.cat((reply_sequence,eos_tok), dim=1).contiguous().view(-1)\n",
    "            src_mask, trg_mask = create_masks(listen_sequence, decoder_input, options)\n",
    "            \n",
    "            encoding = model.encoder(listen_sequence, src_mask) # use the encoder rerepresent the input\n",
    "            encoding, m_scores = model.mem_encoder(encoding, model.current_memory, model.current_memory)\n",
    "            \n",
    "            #decoder_output = model(listen_sequence, decoder_input, src_mask, trg_mask)\n",
    "            decoding = model.decoder(decoder_input, encoding, src_mask, trg_mask)\n",
    "            \n",
    "            memory_vector = decoding[:,-1,:]  # get the last vector as a summary\n",
    "            model.current_memory = model.rmc.update_memory(memory_vector, model.current_memory)\n",
    "            \n",
    "            decoder_logits = model.out(decoding[:,:-1,:])\n",
    "            \n",
    "            decoder_output = decoder_logits.view(-1, decoder_logits.size(-1))\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = F.cross_entropy(decoder_output, decoder_target,\n",
    "                                         ignore_index = options.trg_pad)\n",
    "            batch_loss.backward(retain_graph=True) #batch_loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += batch_loss.item()\n",
    "            \n",
    "        epoch_loss = total_loss/len(conversation_list)\n",
    "        scheduler.step(epoch_loss)\n",
    "        \n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), options.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, epoch, epoch_loss))\n",
    "        total_loss = 0\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.epochs = 20\n",
    "chloe = convo_trainer(conversation_list, chloe, options=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You > my name is fluffy\n",
      "Chloe > <sos> <unk> meowci <unk> <eos>\n",
      "\n",
      "You > hi\n",
      "Chloe > <sos> thank am hi <eos>\n",
      "\n",
      "You > how?\n",
      "Chloe > <sos> <unk> am <unk> <eos>\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f34eb392ad0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtell_chloe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You > \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchloes_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtalk2model_MemRL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtell_chloe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchloe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"bye chloe\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtell_chloe\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"bye ttyl\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchloes_reply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Chloe > '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mchloes_reply\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    tell_chloe = input(\"You > \")\n",
    "    decoder_input, chloes_reply, logprobs = talk2model_MemRL(tell_chloe, chloe, opt, infield, outfield)\n",
    "    if (\"bye chloe\" in tell_chloe or \"bye ttyl\" in chloes_reply):\n",
    "        print('Chloe > '+ chloes_reply + '\\n')\n",
    "        break\n",
    "    else:\n",
    "        print('Chloe > '+ chloes_reply + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
