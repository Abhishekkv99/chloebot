{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.onnx\n",
    "\n",
    "import import_ipynb\n",
    "from Elements import MultiHeadAttention, Norm, FeedForward\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../saved/images/rmc.png\" height=600 width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.]]]) torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "def initial_memory(mem_slots, mem_size, batch_size):\n",
    "    \"\"\"Creates the initial memory.\n",
    "    We should ensure each row of the memory is initialized to be unique,\n",
    "    so initialize the matrix to be the identity. We then pad or truncate\n",
    "    as necessary so that init_state is of size(mem_slots, mem_size).\n",
    "    Args:\n",
    "      mem_slots: rows in memory matrix\n",
    "      mem_size: columns in memory matrix\n",
    "      batch_size: batch size\n",
    "    Returns:\n",
    "      init_state: A truncated or padded identity matrix of size (batch_size,mem_slots, mem_size)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        init_state = torch.stack([torch.eye(mem_slots) for _ in range(batch_size)])\n",
    "\n",
    "    # Pad the matrix with zeros.\n",
    "    if mem_size > mem_slots:\n",
    "      difference = mem_size - mem_slots\n",
    "      pad = torch.zeros((batch_size, mem_slots, difference))\n",
    "      init_state = torch.cat([init_state, pad], -1)\n",
    "    # Truncation. Take the first `self._mem_size` components.\n",
    "    elif mem_size < mem_slots:\n",
    "      init_state = init_state[:, :, :mem_size]\n",
    "    return init_state\n",
    "\n",
    "mem_slots=4\n",
    "mem_size=8\n",
    "batch_size=1\n",
    "memory = initial_memory(mem_slots=mem_slots,mem_size=mem_size,batch_size=batch_size)\n",
    "print(memory, memory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [-1.3526,  0.1950,  0.5727, -0.2224, -0.1243, -0.9050, -2.7317,\n",
      "          -0.3789]]]) torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "input_vector = torch.randn((batch_size,mem_size))\n",
    "print(input_vector.shape)\n",
    "# .unsqueeze(1) because needs to have same dimensions as memory except along dimension we are concatenating \n",
    "memory_plus_input = torch.cat([memory, input_vector.unsqueeze(1)], dim=-2) \n",
    "print(memory_plus_input, memory_plus_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "updatememory = MultiHeadAttention(num_heads=3, emb_dim=8, dim_k=4, dropout=0.0)\n",
    "new_memory, scores = updatememory(memory, memory_plus_input, memory_plus_input)\n",
    "print(new_memory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NormalizeMemory1 = Norm(emb_dim=8)\n",
    "new_mem_norm = NormalizeMemory1(new_memory + memory)\n",
    "new_mem_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP = FeedForward(emb_dim=8, ff_dim=16, dropout=0.2)\n",
    "mem_mlp = MLP(new_mem_norm)\n",
    "NormalizeMemory2 = Norm(emb_dim=8)\n",
    "new_mem_norm2 = NormalizeMemory2(mem_mlp + new_mem_norm)\n",
    "new_mem_norm2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8]) torch.Size([1, 8])\n",
      "torch.Size([1, 4, 8])\n",
      "torch.Size([1, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "print(memory.shape, input_vector.shape)\n",
    "input_stack = torch.stack([input_vector for _ in range(mem_slots)], dim=1)\n",
    "print(input_stack.shape)\n",
    "h_old_x = torch.cat([memory, input_stack], dim = -1)\n",
    "print(h_old_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n",
      "torch.Size([8, 16])\n"
     ]
    }
   ],
   "source": [
    "ZGATE = nn.Linear(mem_size*2, mem_size)\n",
    "z_t = torch.sigmoid(ZGATE(h_old_x)) # (batch size, memory slots, memory size)\n",
    "print(z_t.shape)\n",
    "print(ZGATE.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_memory = (1 - z_t)*memory + z_t*new_mem_norm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_t = \\sigma(W_z \\dot [m_{t - 1},x_t])$$\n",
    "\n",
    "$$m_{t} = (1 - z_t) \\circ m_{t - 1} + z_t \\circ m_{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelMemCore(nn.Module):\n",
    "    \n",
    "    def __init__(self, mem_slots, mem_size, emb_dim, num_heads, dim_k=None, dropout=0.1):\n",
    "        \n",
    "        self.mem_slots = mem_slots\n",
    "        self.mem_size = mem_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_k = dim_k if dim_k else emb_dim // num_heads\n",
    "        self.attn_mem_update = MultiHeadAttention(self.num_heads,self.emb_dim,self.dim_k,self.dropout)\n",
    "        self.NormalizeMemory1 = Norm(emb_dim)\n",
    "        \n",
    "    def initial_memory(self, mem_slots, mem_size, batch_size):\n",
    "        \"\"\"Creates the initial memory.\n",
    "        We should ensure each row of the memory is initialized to be unique,\n",
    "        so initialize the matrix to be the identity. We then pad or truncate\n",
    "        as necessary so that init_state is of size(mem_slots, mem_size).\n",
    "        Args:\n",
    "          mem_slots: rows in memory matrix\n",
    "          mem_size: columns in memory matrix\n",
    "        Returns:\n",
    "          init_state: A truncated or padded identity matrix of size (mem_slots, mem_size).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            init_mem = torch.stack([torch.eye(mem_slots) for _ in range(batch_size)])\n",
    "\n",
    "        # Pad the matrix with zeros.\n",
    "        if mem_size > mem_slots:\n",
    "          difference = mem_size - mem_slots\n",
    "          pad = torch.zeros((batch_size, mem_slots, difference))\n",
    "          init_mem = torch.cat([init_mem, pad], -1)\n",
    "        # Truncation. Take the first `self._mem_size` components.\n",
    "        elif mem_size < mem_slots:\n",
    "          init_mem = init_mem[:, :, :mem_size]\n",
    "        return init_mem\n",
    "        \n",
    "    def update_memory(self, input_vector, cur_memory):\n",
    "        '''\n",
    "        inputs\n",
    "         input_vector (batch_size, mem_size)\n",
    "         cur_memory - current_memory (batch_size, mem_slots, mem_size)\n",
    "        '''\n",
    "        mem_plus_input = torch.cat([cur_memory, input_vector.unsqueeze(1)], dim=-2) \n",
    "        new_mem, scores = self.attn_mem_update(cur_memory, mem_plus_input, mem_plus_input)\n",
    "        new_mem_norm = self.NormalizeMemory1(new_mem + cur_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
