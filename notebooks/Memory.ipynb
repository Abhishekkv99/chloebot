{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.onnx\n",
    "\n",
    "import import_ipynb\n",
    "from Elements import MultiHeadAttention, Norm, FeedForward\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a big picture perspective, the transformer we have built so far, just allows us to map an input sequence to an output sequence. All the layers we put together, essentially are for the purpose of allowing us to do this action -> reaction task. When you are talking to someone, your responses are not just based on what they said last, it is based on what you have said earlier in the conversation, your memory of past conversations, what your currently impression of the person is, and other knowledge of this person and their relationship with the world. This requires us to be able to hold an internal state that persists through time, a memory. In your brain, you have just neurons and the cells that support those neurons. Here we are building a form of neural memory. \n",
    "\n",
    "In the work by [Santoro, A. et al](https://arxiv.org/pdf/1806.01822.pdf) Santoro and collegues built on past work on building memory into neural networks and devised what they called the Relational Memory Core. The image below summarizes this type of neural memory. \n",
    "\n",
    "<img src=\"../saved/images/rmc.png\" height=600 width=800>\n",
    "\n",
    "In (b) of this image notice the 4 x 6 matrix of grey dots labeled memory. This matrix represents a storage of information from the past, past memories. The light grey vector labeled input represents new information from the current time point that we wish to incorporate into our past memories to save them for the future. (c) describes a multi-headed attention mechanism for updating the past memories that is the same as the multiheaded attention we have already learned in the Transformer. In this example the past memories plays the role of the q sequence, the concatenation of the past memories to the input as a new row plays the role of the k and v sequence. The <font color='green'>weights matrix (q_seq_len, k_seq_len)</font> plays the role of the score matrix. The output of the attention mechanism is the updated memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching = False\n",
    "\n",
    "def initial_memory(mem_slots, mem_size, batch_size):\n",
    "    \"\"\"Creates the initial memory.\n",
    "    We should ensure each row of the memory is initialized to be unique,\n",
    "    so initialize the matrix to be the identity. We then pad or truncate\n",
    "    as necessary so that init_state is of size(mem_slots, mem_size).\n",
    "    Args:\n",
    "      mem_slots: rows in memory matrix\n",
    "      mem_size: columns in memory matrix\n",
    "      batch_size: batch size\n",
    "    Returns:\n",
    "      init_state: A truncated or padded identity matrix of size (batch_size,mem_slots, mem_size)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        init_state = torch.stack([torch.eye(mem_slots) for _ in range(batch_size)])\n",
    "\n",
    "    # Pad the matrix with zeros.\n",
    "    if mem_size > mem_slots:\n",
    "      difference = mem_size - mem_slots\n",
    "      pad = torch.zeros((batch_size, mem_slots, difference))\n",
    "      init_state = torch.cat([init_state, pad], -1)\n",
    "    # Truncation. Take the first `self._mem_size` components.\n",
    "    elif mem_size < mem_slots:\n",
    "      init_state = init_state[:, :, :mem_size]\n",
    "    return init_state\n",
    "\n",
    "\n",
    "if teaching:\n",
    "    mem_slots=4\n",
    "    mem_size=8\n",
    "    batch_size=1\n",
    "    memory = initial_memory(mem_slots=mem_slots,mem_size=mem_size,batch_size=batch_size)\n",
    "    print(memory, memory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [-1.3526,  0.1950,  0.5727, -0.2224, -0.1243, -0.9050, -2.7317,\n",
      "          -0.3789]]]) torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "if teaching:\n",
    "    input_vector = torch.randn((batch_size,mem_size))\n",
    "    print(input_vector.shape)\n",
    "    memory_plus_input = torch.cat([memory, input_vector.unsqueeze(1)], dim=-2) \n",
    "    print(memory_plus_input, memory_plus_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "if teaching:\n",
    "    updatememory = MultiHeadAttention(num_heads=3, emb_dim=8, dim_k=4, dropout=0.0)\n",
    "    new_memory, scores = updatememory(memory, memory_plus_input, memory_plus_input)\n",
    "    print(new_memory.shape)\n",
    "    NormalizeMemory1 = Norm(emb_dim=8)\n",
    "    new_mem_norm = NormalizeMemory1(new_memory + memory)\n",
    "    new_mem_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if teaching:\n",
    "    MLP = FeedForward(emb_dim=8, ff_dim=16, dropout=0.2)\n",
    "    mem_mlp = MLP(new_mem_norm)\n",
    "    NormalizeMemory2 = Norm(emb_dim=8)\n",
    "    new_mem_norm2 = NormalizeMemory2(mem_mlp + new_mem_norm)\n",
    "    new_mem_norm2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8]) torch.Size([1, 8])\n",
      "torch.Size([1, 4, 8])\n",
      "torch.Size([1, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "if teaching:\n",
    "    print(memory.shape, input_vector.shape)\n",
    "    input_stack = torch.stack([input_vector for _ in range(mem_slots)], dim=1)\n",
    "    print(input_stack.shape)\n",
    "    h_old_x = torch.cat([memory, input_stack], dim = -1)\n",
    "    print(h_old_x.shape)\n",
    "    ZGATE = nn.Linear(mem_size*2, mem_size)\n",
    "    z_t = torch.sigmoid(ZGATE(h_old_x)) # (batch size, memory slots, memory size)\n",
    "    print(z_t.shape)\n",
    "    print(ZGATE.weight.shape)\n",
    "    new_memory = (1 - z_t)*memory + z_t*new_mem_norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_t = \\sigma(W_z \\dot [m_{t - 1},x_t])$$\n",
    "\n",
    "$$m_{t} = (1 - z_t) \\circ m_{t - 1} + z_t \\circ m_{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelMemCore(nn.Module):\n",
    "    \n",
    "    def __init__(self, mem_slots, mem_size, num_heads, dim_k=None, dropout=0.1):\n",
    "        super(RelMemCore, self).__init__()\n",
    "        self.mem_slots = mem_slots\n",
    "        self.mem_size = mem_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_k = dim_k if dim_k else self.mem_size // num_heads\n",
    "        self.attn_mem_update = MultiHeadAttention(self.num_heads,self.mem_size,self.dim_k,self.dropout)\n",
    "        self.normalizeMemory1 = Norm(self.mem_size)\n",
    "        self.normalizeMemory2 = Norm(self.mem_size)\n",
    "        self.MLP = FeedForward(self.mem_size, ff_dim=self.mem_size*2, dropout=dropout)\n",
    "        self.ZGATE = nn.Linear(self.mem_size*2, self.mem_size)\n",
    "        \n",
    "    def initial_memory(self, batch_size):\n",
    "        \"\"\"Creates the initial memory.\n",
    "        We should ensure each row of the memory is initialized to be unique,\n",
    "        so initialize the matrix to be the identity. We then pad or truncate\n",
    "        as necessary so that init_state is of size (mem_slots, mem_size).\n",
    "        Args:\n",
    "          mem_slots: rows in memory matrix\n",
    "          mem_size: columns in memory matrix\n",
    "        Returns:\n",
    "          init_state: A truncated or padded identity matrix of size (mem_slots, mem_size).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            init_mem = torch.stack([torch.eye(self.mem_slots) for _ in range(batch_size)])\n",
    "\n",
    "        # Pad the matrix with zeros.\n",
    "        if self.mem_size > self.mem_slots:\n",
    "          difference = self.mem_size - self.mem_slots\n",
    "          pad = torch.zeros((batch_size, self.mem_slots, difference))\n",
    "          init_mem = torch.cat([init_mem, pad], -1)\n",
    "        # Truncation. Take the first `self._mem_size` components.\n",
    "        elif self.mem_size < self.mem_slots:\n",
    "          init_mem = init_mem[:, :, :self.mem_size]\n",
    "        return init_mem\n",
    "        \n",
    "    def update_memory(self, input_vector, cur_memory):\n",
    "        '''\n",
    "        inputs\n",
    "         input_vector (batch_size, mem_size)\n",
    "         cur_memory - current_memory (batch_size, mem_slots, mem_size)\n",
    "        output\n",
    "         new_memory (batch_size, mem_slots, mem_size)\n",
    "        '''\n",
    "        mem_plus_input = torch.cat([cur_memory, input_vector.unsqueeze(1)], dim=-2) \n",
    "        new_mem, scores = self.attn_mem_update(cur_memory, mem_plus_input, mem_plus_input)\n",
    "        new_mem_norm = self.normalizeMemory1(new_mem + cur_memory)\n",
    "        mem_mlp = self.MLP(new_mem_norm)\n",
    "        new_mem_norm2 = self.normalizeMemory2(mem_mlp + new_mem_norm)\n",
    "        input_stack = torch.stack([input_vector for _ in range(self.mem_slots)], dim=1)\n",
    "        h_old_x = torch.cat([cur_memory, input_stack], dim = -1)\n",
    "        z_t = torch.sigmoid(self.ZGATE(h_old_x)) # (batch size, memory slots, memory size)\n",
    "        new_memory = (1 - z_t)*memory + z_t*new_mem_norm2\n",
    "        return new_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmc = RelMemCore(mem_slots=4, mem_size=8, num_heads=3)\n",
    "cur_mem = rmc.initial_memory(batch_size=1)\n",
    "input_vector = torch.randn((batch_size,mem_size))\n",
    "new_memory = rmc.update_memory(input_vector, cur_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.]]]) torch.Size([1, 4, 8])\n",
      "tensor([[[ 1.4589, -0.5712, -0.2194, -0.2879, -0.3375,  0.1780, -0.1343,\n",
      "           0.3922],\n",
      "         [ 0.4755,  0.9693, -0.3751, -0.4355, -0.6177,  0.1510, -0.4546,\n",
      "           0.7467],\n",
      "         [ 0.3362, -0.7275,  1.2541, -0.3972, -0.4413,  0.0264, -0.1823,\n",
      "           0.5694],\n",
      "         [ 0.4066, -0.6822, -0.2506,  1.2347, -0.4806,  0.2021, -0.2281,\n",
      "           0.5219]]], grad_fn=<AddBackward0>) torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "print(cur_mem, cur_mem.shape)\n",
    "print(new_memory, new_memory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_mem_update.q_linear.weight torch.Size([6, 8])\n",
      "attn_mem_update.q_linear.bias torch.Size([6])\n",
      "attn_mem_update.k_linear.weight torch.Size([6, 8])\n",
      "attn_mem_update.k_linear.bias torch.Size([6])\n",
      "attn_mem_update.v_linear.weight torch.Size([6, 8])\n",
      "attn_mem_update.v_linear.bias torch.Size([6])\n",
      "attn_mem_update.out.weight torch.Size([8, 6])\n",
      "attn_mem_update.out.bias torch.Size([8])\n",
      "normalizeMemory1.alpha torch.Size([8])\n",
      "normalizeMemory1.bias torch.Size([8])\n",
      "normalizeMemory2.alpha torch.Size([8])\n",
      "normalizeMemory2.bias torch.Size([8])\n",
      "MLP.linear_1.weight torch.Size([16, 8])\n",
      "MLP.linear_1.bias torch.Size([16])\n",
      "MLP.linear_2.weight torch.Size([8, 16])\n",
      "MLP.linear_2.bias torch.Size([8])\n",
      "ZGATE.weight torch.Size([8, 16])\n",
      "ZGATE.bias torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for name, param in rmc.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 8])\n",
      "torch.Size([6])\n",
      "torch.Size([6, 8])\n",
      "torch.Size([6])\n",
      "torch.Size([6, 8])\n",
      "torch.Size([6])\n",
      "torch.Size([8, 6])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "torch.Size([16, 8])\n",
      "torch.Size([16])\n",
      "torch.Size([8, 16])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 16])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for parameter in rmc.parameters():\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
