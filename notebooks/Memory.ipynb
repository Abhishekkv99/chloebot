{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MoveData.ipynb\n",
      "importing Jupyter notebook from Elements.ipynb\n",
      "importing Jupyter notebook from EncoderDecoder.ipynb\n",
      "importing Jupyter notebook from Talk.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carsonlam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import Options, json2datatools, num_batches, nopeak_mask, create_masks, load_subset_weights\n",
    "from Elements import MultiHeadAttention, Norm, FeedForward\n",
    "from EncoderDecoder import Encoder, Decoder\n",
    "from Talk import talk_to_chloe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Memory Core\n",
    "\n",
    "From a big picture perspective, the transformer we have built so far, just allows us to map an input sequence to an output sequence. All the layers we put together, essentially are for the purpose of allowing us to do this action -> reaction task. When you are talking to someone, your responses are not just based on what they said last, it is based on what you have said earlier in the conversation, your memory of past conversations, what your currently impression of the person is, and other knowledge of this person and their relationship with the world. This requires us to be able to hold an internal state that persists through time, a memory. In your brain, you have just neurons and the cells that support those neurons. Here we are building a form of neural memory. \n",
    "\n",
    "In the work by [Santoro, A. et al](https://arxiv.org/pdf/1806.01822.pdf) from Deepmind, Santoro and collegues built on past work on building memory into neural networks and devised what they called the Relational Memory Core (RMC). The image below summarizes this type of neural memory. \n",
    "\n",
    "<img src=\"../saved/images/RMC_Overview.png\" height=400 width=600>\n",
    "\n",
    "The image above shows a high level graphic of how the memory, in the form of a matrix called Previous Memory, is combined with current experience in the form of an input vector, using attention, A, into a Next or Updated memory. \n",
    "\n",
    "The RMC has three high level steps, with a residual and normalization layer between each one. They are 1. Attention, 2. Multi-Layer-Perceptron (MLP) and 3. Gating. \n",
    "\n",
    "The cell below creates a blank initial memory matrix to which an agent will fill with it's memories. The memory matrix has `mem_slots` number of rows and `mem_size` number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching = False\n",
    "\n",
    "def initial_memory(mem_slots, mem_size, batch_size):\n",
    "    \"\"\"Creates the initial memory.\n",
    "    We should ensure each row of the memory is initialized to be unique,\n",
    "    so initialize the matrix to be the identity. We then pad or truncate\n",
    "    as necessary so that init_state is of size(mem_slots, mem_size).\n",
    "    Args:\n",
    "      mem_slots: rows in memory matrix\n",
    "      mem_size: columns in memory matrix\n",
    "      batch_size: batch size\n",
    "    Returns:\n",
    "      init_state: A truncated or padded identity matrix of size (batch_size,mem_slots, mem_size)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        init_state = torch.stack([torch.eye(mem_slots) for _ in range(batch_size)])\n",
    "\n",
    "    # Pad the matrix with zeros.\n",
    "    if mem_size > mem_slots:\n",
    "      difference = mem_size - mem_slots\n",
    "      pad = torch.zeros((batch_size, mem_slots, difference))\n",
    "      init_state = torch.cat([init_state, pad], -1)\n",
    "    # Truncation. Take the first `self._mem_size` components.\n",
    "    elif mem_size < mem_slots:\n",
    "      init_state = init_state[:, :, :mem_size]\n",
    "    return init_state\n",
    "\n",
    "\n",
    "if teaching:\n",
    "    mem_slots=4\n",
    "    mem_size=8\n",
    "    batch_size=1\n",
    "    memory = initial_memory(mem_slots=mem_slots,mem_size=mem_size,batch_size=batch_size)\n",
    "    print(\"Initial Memory\")\n",
    "    print(memory, memory.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this image notice the 4 x 6 matrix of grey dots labeled memory. This matrix represents a storage of information from the past, past memories. The light grey vector labeled input represents new information from the current time point that we wish to incorporate into our past memories to save them for the future. \n",
    "\n",
    "<img src=\"../saved/images/RMC_MHDPA.png\"  height=600 width=800>\n",
    "\n",
    "The lower panel describes a multi-headed attention mechanism for updating the past memories into the updated memories. The sequence of operations is the same as the multiheaded attention we have already learned in the Transformer. \n",
    "\n",
    "In the Transformer we used this type of attention 3 ways:\n",
    "\n",
    "1. For Encoder-Encoder Attention in which the source sequence attends to itself\n",
    "\n",
    "2. For Self-Attention in which the Decoder Output attends to the Decoder Inputs so far\n",
    "\n",
    "3. Decoder-Encoder Attention in which the Decoder Output attends to the Encoder Ouputs\n",
    "\n",
    "recall the forward method of class MultiHeadAttention\n",
    "\n",
    "`def forward(self, q, k, v, mask=None, explain=False):`\n",
    "\n",
    "q had shape (batch size, q_sequence length, embedding dimensions), the output of the MultiHeadAttention will be the same shape as q. In the RMC q is our previous memory of shape (batch size, mem_slots, mem_size). The updated memory will have the same shape as the previous memory after it attends to `mem_plus_input`, which is a matrix that includes previous memory inside it, but it has an extra row, that extra row is the input vector that represents the current experience to be incorporated into memory. `mem_plus_input` has shape (batch size, mem_slots + 1, mem_size)\n",
    "\n",
    "Using Decoder-Encoder Attention as an analogy. The previous memory plays the role of the sequence that the q projection is derived from, the Decoder Input. The concatenation of the previous memory with the input as a new row is analogous to the sequence that the k and v projections are derived from, the encoder output. \n",
    "\n",
    "The <font color='green'>weights matrix (q_seq_len, k_seq_len)</font> is analogous to the score matrix in the transformer. w1,2 in the diagram is the amount of attention that the 1st slot of the previous memory should pay to the 2nd slot in `mem_plus_input`. The scores are normalized just as in the Transformer using a softmax and dividing by \n",
    "\n",
    "$$\\sqrt(memory size)$$\n",
    "\n",
    "Looking at the <font color='green'>Normalized Weights</font> in the diagram, notice that in the bottom row, the row selected by the grey rectangle, the first green dot is the most <font color='green'>green</font>. This row of length 5 will be dot producted with each column of the yellow <font color='yellow'>Values</font> matrix to calculate each element of the bottom row of the new updated memory matrix. The significance of the first green dot being most green is that the row that is to be weighted the most, the row in the yellow <font color='yellow'>Values</font> matrix that the bottom row of the updated memory will most be similar to, is the first row of the yellow <font color='yellow'>Values</font> matrix.  \n",
    "\n",
    "The cell below performs this attention step. After the Attention step notice the line `new_mem_norm = NormalizeMemory1(new_memory + memory)`. This line is both the normalization step and also the residual. Residual is simply adding a vector to the version of itself before it was modified, in this case, adding the memory matrix to the version of itself before attention was applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector that represents your current experience\n",
      "tensor([[ 0.2986, -0.4072,  1.3784,  0.8294,  1.2003,  0.1381, -1.1695,  0.2096]]) torch.Size([1, 8])\n",
      "--------------------------------------------------------------\n",
      "Previous Memory with the new input as the bottom row\n",
      "tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2986, -0.4072,  1.3784,  0.8294,  1.2003,  0.1381, -1.1695,\n",
      "           0.2096]]]) torch.Size([1, 5, 8])\n",
      "--------------------------------------------------------------\n",
      "Next Memory after Attention Step but Before MLP and Gating Steps\n",
      "tensor([[[-0.0562,  0.0282,  0.0104,  0.4376,  0.0601, -0.2478,  0.2248,\n",
      "           0.4245],\n",
      "         [-0.0606,  0.0267,  0.0160,  0.4267,  0.0666, -0.2469,  0.2214,\n",
      "           0.4173],\n",
      "         [-0.0596,  0.0284,  0.0129,  0.4289,  0.0650, -0.2438,  0.2214,\n",
      "           0.4158],\n",
      "         [-0.0558,  0.0237,  0.0145,  0.4309,  0.0622, -0.2488,  0.2240,\n",
      "           0.4220]]], grad_fn=<AddBackward0>) torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "if teaching:\n",
    "    print(\"Input vector that represents your current experience\")\n",
    "    input_vector = torch.randn((batch_size,mem_size))\n",
    "    print(input_vector ,input_vector.shape)\n",
    "    memory_plus_input = torch.cat([memory, input_vector.unsqueeze(1)], dim=-2) \n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    print(\"Previous Memory with the new input as the bottom row\")\n",
    "    print(memory_plus_input, memory_plus_input.shape)\n",
    "    updatememory = MultiHeadAttention(num_heads=3, emb_dim=8, dim_k=4, dropout=0.0)\n",
    "    new_memory, scores = updatememory(memory, memory_plus_input, memory_plus_input)\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    print(\"Next Memory after Attention Step but Before MLP and Gating Steps\")\n",
    "    print(new_memory, new_memory.shape)\n",
    "    NormalizeMemory1 = Norm(emb_dim=8)\n",
    "    new_mem_norm = NormalizeMemory1(new_memory + memory)\n",
    "    new_mem_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This net step applies the same Feed Forward Neural Network to each memory slot of the updated memory and performs the 2nd residual + normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8800, -0.6799, -0.9192,  0.6317, -0.5448, -1.0849,  0.2395,\n",
      "           0.4777],\n",
      "         [-0.7467,  1.6685, -0.8662,  0.8525, -0.5407, -1.2059,  0.2150,\n",
      "           0.6234],\n",
      "         [-0.4988, -0.7131,  1.3087,  0.8573, -0.7955, -1.5029,  0.6804,\n",
      "           0.6638],\n",
      "         [-0.3512, -0.3280, -0.6274,  2.0999, -0.7908, -0.9259,  0.2931,\n",
      "           0.6304]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if teaching:\n",
    "    MLP = FeedForward(emb_dim=8, ff_dim=16, dropout=0.2)\n",
    "    mem_mlp = MLP(new_mem_norm)\n",
    "    NormalizeMemory2 = Norm(emb_dim=8)\n",
    "    new_mem_norm2 = NormalizeMemory2(mem_mlp + new_mem_norm)\n",
    "    print(new_mem_norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is the Gating step. This step is inspired by the Gated Recurrent Unit (Cho et al., 2014) and [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) (Hochreiter & Schmidhuber 1997). We use the gate (z_t) for a slightly different reason than the [vanishing gradient problem](https://stats.stackexchange.com/questions/185639/how-does-lstm-prevent-the-vanishing-gradient-problem), the reason it is used for recurrent neural networks. It is essentially a way to make element-wise decisions to add the same amount of change that is removed from the previous state. \n",
    "\n",
    "$$z_t = \\sigma(W_z \\dot [m_{t - 1},x_t])$$\n",
    "\n",
    "$$m_{t} = (1 - z_t) \\circ m_{t - 1} + z_t \\circ m_{t}$$\n",
    "\n",
    "*z_t* is a matrix that is the same shape as the memory matrix. Since it comes out of a [sigmoid function](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6) its vales are all between 0 and 1. \n",
    "\n",
    "Suppose element ij of z_t is 0.2. This means that for that element (1 - z_t) = 0.8 of the element will come from the previous memory m_(t -1), and z_t or 0.2 of the element will come from the new updated memory m_(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8]) torch.Size([1, 8])\n",
      "torch.Size([1, 4, 8])\n",
      "torch.Size([1, 4, 16])\n",
      "torch.Size([1, 4, 8])\n",
      "torch.Size([8, 16])\n"
     ]
    }
   ],
   "source": [
    "if teaching:\n",
    "    print(memory.shape, input_vector.shape)\n",
    "    input_stack = torch.stack([input_vector for _ in range(mem_slots)], dim=1)\n",
    "    print(input_stack.shape)\n",
    "    h_old_x = torch.cat([memory, input_stack], dim = -1)\n",
    "    print(h_old_x.shape)\n",
    "    ZGATE = nn.Linear(mem_size*2, mem_size)\n",
    "    z_t = torch.sigmoid(ZGATE(h_old_x)) # (batch size, memory slots, memory size)\n",
    "    print(z_t.shape)\n",
    "    print(ZGATE.weight.shape)\n",
    "    new_memory = (1 - z_t)*memory + z_t*new_mem_norm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Relational Memory Core *class*\n",
    "Putting it all together to create our Relational Memory Core (RelMemCore) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelMemCore(nn.Module):\n",
    "    \n",
    "    def __init__(self, mem_slots, mem_size, num_heads, dim_k=None, dropout=0.1):\n",
    "        super(RelMemCore, self).__init__()\n",
    "        self.mem_slots = mem_slots\n",
    "        self.mem_size = mem_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_k = dim_k if dim_k else self.mem_size // num_heads\n",
    "        self.attn_mem_update = MultiHeadAttention(self.num_heads,self.mem_size,self.dim_k,self.dropout)\n",
    "        self.normalizeMemory1 = Norm(self.mem_size)\n",
    "        self.normalizeMemory2 = Norm(self.mem_size)\n",
    "        self.MLP = FeedForward(self.mem_size, ff_dim=self.mem_size*2, dropout=dropout)\n",
    "        self.ZGATE = nn.Linear(self.mem_size*2, self.mem_size)\n",
    "        \n",
    "    def initial_memory(self, batch_size):\n",
    "        \"\"\"Creates the initial memory.\n",
    "        TO ensure each row of the memory is initialized to be unique, \n",
    "        initialize the matrix as the identity then pad or truncate\n",
    "        so that init_state is of size (mem_slots, mem_size).\n",
    "        Args:\n",
    "          batch size\n",
    "        Returns:\n",
    "          init_mem: A truncated or padded identity matrix of size (mem_slots, mem_size)\n",
    "          remember_vector: (1, self.mem_size)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            init_mem = torch.stack([torch.eye(self.mem_slots) for _ in range(batch_size)])\n",
    "            \n",
    "        # Pad the matrix with zeros.\n",
    "        if self.mem_size > self.mem_slots:\n",
    "          difference = self.mem_size - self.mem_slots\n",
    "          pad = torch.zeros((batch_size, self.mem_slots, difference))\n",
    "          init_mem = torch.cat([init_mem, pad], -1)\n",
    "        # Truncation. Take the first `self._mem_size` components.\n",
    "        elif self.mem_size < self.mem_slots:\n",
    "          init_mem = init_mem[:, :, :self.mem_size]\n",
    "        \n",
    "        return init_mem\n",
    "        \n",
    "    def update_memory(self, input_vector, prev_memory):\n",
    "        '''\n",
    "        inputs\n",
    "         input_vector (batch_size, mem_size)\n",
    "         prev_memory - previous or past memory (batch_size, mem_slots, mem_size)\n",
    "        output\n",
    "         next_memory - updated memory (batch_size, mem_slots, mem_size)\n",
    "        '''\n",
    "        mem_plus_input = torch.cat([prev_memory, input_vector.unsqueeze(1)], dim=-2) \n",
    "        new_mem, scores = self.attn_mem_update(prev_memory, mem_plus_input, mem_plus_input)\n",
    "        new_mem_norm = self.normalizeMemory1(new_mem + prev_memory)\n",
    "        mem_mlp = self.MLP(new_mem_norm)\n",
    "        new_mem_norm2 = self.normalizeMemory2(mem_mlp + new_mem_norm)\n",
    "        input_stack = torch.stack([input_vector for _ in range(self.mem_slots)], dim=1)\n",
    "        h_old_x = torch.cat([prev_memory, input_stack], dim = -1)\n",
    "        z_t = torch.sigmoid(self.ZGATE(h_old_x)) # (batch size, memory slots, memory size)\n",
    "        next_memory = (1 - z_t)*prev_memory + z_t*new_mem_norm2\n",
    "        return next_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.]]]) torch.Size([1, 4, 8])\n",
      "------------------------------------------\n",
      "tensor([[[-0.1236,  0.0346, -1.3370, -0.3710,  0.1886,  0.3484,  1.3409,\n",
      "           1.0054]]], grad_fn=<RepeatBackward>) torch.Size([1, 1, 8])\n",
      "------------------------------------------\n",
      "tensor([[[ 1.6770, -0.6598,  0.0461, -0.1969, -0.3158,  0.1949, -0.2053,\n",
      "          -0.0955],\n",
      "         [ 0.5045,  1.3225, -0.1530, -0.3141, -0.7260,  0.2137, -0.2168,\n",
      "          -0.2624],\n",
      "         [ 0.3497, -0.8006,  1.4461, -0.0663, -0.1956,  0.1075, -0.1415,\n",
      "          -0.2532],\n",
      "         [ 0.3222, -0.7927,  0.0070,  1.6306, -0.1580,  0.1044, -0.3018,\n",
      "          -0.2089]]], grad_fn=<AddBackward0>) torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "if teaching:\n",
    "    \n",
    "    rmc = RelMemCore(mem_slots=4, mem_size=8, num_heads=3)\n",
    "    cur_mem = rmc.initial_memory(batch_size=1)\n",
    "    input_vector = torch.randn((batch_size,mem_size))\n",
    "    new_memory = rmc.update_memory(input_vector, cur_mem)\n",
    "    print(cur_mem, cur_mem.shape)\n",
    "    print(\"------------------------------------------\")\n",
    "    print(new_memory, new_memory.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chloe, but with memory\n",
    "\n",
    "The way that our new model will take into account memory is by re-representing the encoding based on this memory using . . . you guessed it, attention. \n",
    "\n",
    "`m_output, m_scores = self.mem_encoder(e_output,self.current_memory,self.current_memory)`\n",
    "\n",
    "We still need to decide when in the conversation to update the memory and implement that update into the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTransformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, num_heads, mem_slots, dropout):\n",
    "        \n",
    "        super(MemoryTransformer, self).__init__() \n",
    "        \n",
    "        self.mem_slots = mem_slots\n",
    "        self.mem_size = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_k = self.mem_size // self.num_heads\n",
    "        \n",
    "        self.encoder = Encoder(in_vocab_size, emb_dim, n_layers, num_heads, dropout)\n",
    "        self.rmc = RelMemCore(mem_slots, mem_size=emb_dim, num_heads=num_heads)\n",
    "        \n",
    "        self.current_memory = self.rmc.initial_memory(batch_size=1)\n",
    "        \n",
    "        self.mem_encoder = MultiHeadAttention(num_heads,self.mem_size,self.dim_k,dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, num_heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "             \n",
    "    def forward(self, src_seq, trg_seq, src_mask, trg_mask):\n",
    "        e_output = self.encoder(src_seq, src_mask)\n",
    "        m_output, m_scores = self.mem_encoder(e_output,self.current_memory,self.current_memory)\n",
    "        d_output = self.decoder(trg_seq, m_output, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if teaching:\n",
    "    opt = Options(batchsize=2, device = torch.device(\"cpu\"), epochs=25, lr=0.01, \n",
    "                  beam_width=3, max_len = 25, save_path = '../saved/weights/model_weights')\n",
    "\n",
    "    data_iter, infield, outfield, opt = json2datatools(path='../saved/pairs.json', opt=opt)\n",
    "\n",
    "    emb_dim, n_layers, num_heads, mem_slots, dropout = 32, 3, 8, 4, 0.01 \n",
    "    chloe = MemoryTransformer(len(infield.vocab), len(outfield.vocab), \n",
    "                              emb_dim, n_layers, num_heads, mem_slots, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meowci beaucoup\n"
     ]
    }
   ],
   "source": [
    "def load_subset_weights(whole_model, opt):\n",
    "    '''\n",
    "    This function allows you to load saved weights from a saved model that is a subset of your model\n",
    "    It looks for the named parameters that match and loads those but will not crash trying to load\n",
    "    parameters that dont have a matching name\n",
    "    '''\n",
    "    subset_model_dict = torch.load(opt.save_path)\n",
    "    whole_model_dict = whole_model.state_dict() \n",
    "    for name, param in whole_model_dict.items(): \n",
    "        if name in subset_model_dict:\n",
    "            whole_model_dict[name].copy_(subset_model_dict[name])\n",
    "\n",
    "if teaching:\n",
    "    # This function allows you to load saved weights from a saved model that is a subset of your model\n",
    "    load_subset_weights(chloe, opt)\n",
    "    # talk_to_chloe only uses the encoder and decoder, it does not use the memory encoder \n",
    "    print(talk_to_chloe(\"how?\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lesson is about teaching our neural network to work towards a goal, open `Reinforce.ipynb` for the next part in our intellectual adventure\n",
    "\n",
    "\n",
    "## How can I help you or get help from you?\n",
    "\n",
    "[Support *ChloeRobotics* on Patreon](https://www.patreon.com/chloerobotics)\n",
    "\n",
    "email chloe.the.robot [at] gmail [dot] com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
