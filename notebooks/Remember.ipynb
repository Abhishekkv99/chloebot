{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MoveData.ipynb\n",
      "importing Jupyter notebook from EncoderDecoder.ipynb\n",
      "importing Jupyter notebook from Elements.ipynb\n",
      "importing Jupyter notebook from Talk.ipynb\n",
      "importing Jupyter notebook from Trainer.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carsonlam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import *\n",
    "from EncoderDecoder import *\n",
    "from Talk import *\n",
    "from Trainer import *\n",
    "from Talk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MemoryTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTransformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, \n",
    "                 heads, mem_slots, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = None\n",
    "        dim_k = emb_dim // heads\n",
    "        self.mem_slots = mem_slots\n",
    "        \n",
    "        self.encoder = Encoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "        \n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        self.memory = torch.randn((self.mem_slots, emb_dim))\n",
    "        '''\n",
    "        mem_mask = np.ones((1,self.mem_slots)).astype('uint8')\n",
    "        self.mem_mask =  torch.from_numpy(mem_mask) == 1\n",
    "        \n",
    "        self.mem_update = MultiHeadAttention(heads, emb_dim, dim_k, dropout)\n",
    "        self.z_gate = nn.Linear(emb_dim, emb_dim)\n",
    "        self.NormalizeMemory = Norm(emb_dim)\n",
    "        '''\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "        # needed for truncated BPTT, called at every batch forward pass\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        \n",
    "    def batch_memory(self, in_toks):\n",
    "        self.batch_size = in_toks.size(0)\n",
    "        self.memory = torch.stack([self.memory for _ in range(self.batch_size)])\n",
    "        #self.mem_mask = torch.stack([self.mem_mask for _ in range(self.batch_size)])\n",
    "        print(\"setting batch size to \", self.batch_size)\n",
    "        \n",
    "    def update_memory(self):\n",
    "        #mem_dialogue = torch.cat([self.memory, self.e_output, self.d_output], dim=-2) \n",
    "        #mem_dialogue = torch.cat([self.memory, self.d_output], dim=-2)\n",
    "        #new_memory, scores = self.mem_update(self.memory, mem_dialogue, mem_dialogue)\n",
    "        #new_mem_norm = self.NormalizeMemory(new_memory + self.memory)\n",
    "        #z_t = torch.sigmoid(self.z_gate(self.memory)) # (batch size, memory slots, memory size)\n",
    "        #self.memory = (1 - z_t)*self.memory + z_t*new_mem_norm\n",
    "        self.memory = self.e_output\n",
    "\n",
    "    def forward(self, in_toks, in_mask, out_seq, out_mask):\n",
    "        self.memory = self.repackage_hidden(self.memory)\n",
    "        if self.batch_size == None: self.batch_memory(in_toks)\n",
    "            \n",
    "        self.e_output = self.encoder(in_toks, in_mask)\n",
    "\n",
    "        mem_en_vecs = torch.cat([self.memory, self.e_output], dim=-2) \n",
    "        mem_en_mask = torch.from_numpy(np.ones((self.batch_size, 1, mem_en_vecs.size(-2)))) == 1\n",
    "        self.d_output = self.decoder(out_seq, out_mask, mem_en_vecs, mem_en_mask)\n",
    "        output = self.out(self.d_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <sos> <sos> <pad> snuggles silly silly its\n"
     ]
    }
   ],
   "source": [
    "opt = Options(batchsize=1, device = torch.device(\"cpu\"), epochs=20, lr=0.01, \n",
    "              max_len = 25, save_path = '../saved/weights/memory_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../saved/memory.json', opt=opt)\n",
    "\n",
    "emb_dim, n_layers, heads, mem_slots, dropout = 32, 2, 8, 1, 0.01 \n",
    "chloe = MemoryTransformer(len(infield.vocab), len(outfield.vocab), emb_dim, n_layers, heads, mem_slots, dropout)\n",
    "\n",
    "#load_subset_weights(chloe, opt)\n",
    "print(talk_to_chloe(\"my name is fluffy\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n",
      "0m: epoch 0 loss = 0.263\n",
      "0m: epoch 1 loss = 0.136\n",
      "0m: epoch 2 loss = 0.173\n",
      "0m: epoch 3 loss = 0.277\n",
      "0m: epoch 4 loss = 0.193\n",
      "0m: epoch 5 loss = 0.148\n",
      "0m: epoch 6 loss = 0.152\n",
      "0m: epoch 7 loss = 0.113\n",
      "0m: epoch 8 loss = 0.131\n",
      "0m: epoch 9 loss = 0.080\n",
      "0m: epoch 10 loss = 0.064\n",
      "0m: epoch 11 loss = 0.167\n",
      "0m: epoch 12 loss = 0.122\n",
      "0m: epoch 13 loss = 0.100\n",
      "0m: epoch 14 loss = 0.159\n",
      "0m: epoch 15 loss = 0.081\n",
      "0m: epoch 16 loss = 0.087\n"
     ]
    }
   ],
   "source": [
    "def trainer(model, data_iterator, options, optimizer, scheduler):\n",
    "\n",
    "    if torch.cuda.is_available() and options.device == torch.device(\"cuda:0\"):\n",
    "        print(\"a GPU was detected, model will be trained on GPU\")\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        print(\"training on cpu\")\n",
    "\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    best_loss = 100\n",
    "    for epoch in range(options.epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(data_iterator): \n",
    "            src = batch.listen.transpose(0,1)\n",
    "            trg = batch.reply.transpose(0,1)\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, options)\n",
    "            preds = model(src, src_mask, trg_input, trg_mask)\n",
    "            #model.update_memory() # Update Memory \n",
    "            \n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
    "                                         ys, ignore_index = options.trg_pad)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "        epoch_loss = total_loss/(num_batches(data_iterator)+1)\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), options.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, epoch, epoch_loss))\n",
    "        total_loss = 0\n",
    "\n",
    "    return model\n",
    "\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=3)\n",
    "\n",
    "chloe = trainer(chloe, data_iter, opt, optimizer, scheduler)\n",
    "print(talk_to_chloe(\"my name is snuggles\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory() # Update Memory \n",
    "print(talk_to_chloe(\"what is my name?\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 0.446\n",
      "0m: epoch 1 loss = 0.719\n",
      "0m: epoch 2 loss = 0.354\n",
      "0m: epoch 3 loss = 0.475\n",
      "0m: epoch 4 loss = 0.280\n",
      "0m: epoch 5 loss = 0.335\n",
      "0m: epoch 6 loss = 0.250\n",
      "0m: epoch 7 loss = 0.215\n",
      "0m: epoch 8 loss = 0.194\n",
      "0m: epoch 9 loss = 0.190\n",
      "0m: epoch 10 loss = 0.190\n",
      "0m: epoch 11 loss = 0.193\n",
      "0m: epoch 12 loss = 0.192\n",
      "0m: epoch 13 loss = 0.193\n",
      "0m: epoch 14 loss = 0.191\n",
      "0m: epoch 15 loss = 0.191\n",
      "0m: epoch 16 loss = 0.193\n",
      "0m: epoch 17 loss = 0.191\n",
      "0m: epoch 18 loss = 0.191\n",
      "0m: epoch 19 loss = 0.190\n",
      "> my name is fluffy > hello fluffy !\n",
      "> what is my name? > hello bobo !\n",
      "> my name is snuggles > hello snuggles !\n",
      "> what is my name? > hello bobo !\n",
      "> my name is bobo > its bobo silly\n",
      "> what is my name? > its bobo silly\n"
     ]
    }
   ],
   "source": [
    "opt = Options(batchsize=1, device = torch.device(\"cpu\"), epochs=20, lr=0.01, \n",
    "              max_len = 25, save_path = '../saved/weights/memory_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../saved/memory.json', opt=opt)\n",
    "\n",
    "conversation_list = [\n",
    "    {\"listen\":\"my name is fluffy\", \"reply\":\"hello fluffy!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"its fluffy silly\"},\n",
    "    {\"listen\":\"my name is snuggles\", \"reply\":\"hello snuggles!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"its snuggles silly\"},\n",
    "    {\"listen\":\"my name is bobo\", \"reply\":\"hello bobo!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"its bobo silly\"},\n",
    "                    ]\n",
    "\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=4)\n",
    "\n",
    "sos_tok = torch.LongTensor([[outfield.vocab.stoi['<sos>']]]) \n",
    "eos_tok = torch.LongTensor([[outfield.vocab.stoi['<eos>']]]) \n",
    "\n",
    "chloe.train()\n",
    "start = time.time()\n",
    "best_loss = 100\n",
    "opt.epochs = 20\n",
    "for epoch in range(opt.epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(conversation_list)):\n",
    "        listen_string = conversation_list[i][\"listen\"]\n",
    "        reply_string = conversation_list[i][\"reply\"]\n",
    "        listen_toks = string2tensor(listen_string, infield)\n",
    "        reply_toks = string2tensor(reply_string, outfield)\n",
    "        reply_start = torch.cat((sos_tok,reply_toks), dim=1)\n",
    "        reply_labels = torch.cat((reply_toks,eos_tok), dim=1).contiguous().view(-1)\n",
    "        \n",
    "        listen_mask, reply_mask = create_masks(listen_toks, reply_start, opt)\n",
    "        \n",
    "        logits = chloe(listen_toks, listen_mask, reply_start, reply_mask)\n",
    "        \n",
    "        chloe.update_memory() # Update Memory\n",
    "        \n",
    "        flat_logits = logits.view(-1, logits.size(-1))\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = F.cross_entropy(flat_logits, reply_labels, ignore_index = opt.trg_pad)\n",
    "\n",
    "        batch_loss.backward() #batch_loss.backward(retain_graph=True) #\n",
    "        torch.nn.utils.clip_grad_norm_(chloe.parameters(), max_norm = 0.5) \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    epoch_loss = total_loss/len(conversation_list)\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(chloe.state_dict(), opt.save_path)\n",
    "    print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, \n",
    "                                        epoch, epoch_loss))\n",
    "    total_loss = 0\n",
    "\n",
    "    \n",
    "chloe.eval()\n",
    "print(\"> my name is fluffy >\", talk_to_chloe(\"my name is fluffy\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> what is my name? >\", talk_to_chloe(\"what is my name?\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> my name is snuggles >\", talk_to_chloe(\"my name is snuggles\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> what is my name? >\", talk_to_chloe(\"what is my name?\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> my name is bobo >\", talk_to_chloe(\"my name is bobo\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(\"> what is my name? >\", talk_to_chloe(\"what is my name?\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to train the memory. How do we do this? we need to talk to the model and allow it to accumulate at least one cycle of conversation, then teach it to respond correctly given the previous listen-reply exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meowci beaucoup !\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank am hi\n",
      "<unk> meowci <unk>\n",
      "thank meowci hi\n",
      "<unk> meowci <unk>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You > my name is fluffy\n",
      "Chloe > <sos> <unk> meowci <unk> <eos>\n",
      "\n",
      "You > hi\n",
      "Chloe > <sos> thank am hi <eos>\n",
      "\n",
      "You > how?\n",
      "Chloe > <sos> <unk> am <unk> <eos>\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f34eb392ad0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtell_chloe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You > \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchloes_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtalk2model_MemRL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtell_chloe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchloe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"bye chloe\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtell_chloe\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"bye ttyl\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchloes_reply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Chloe > '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mchloes_reply\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
