{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MoveData.ipynb\n",
      "importing Jupyter notebook from Elements.ipynb\n",
      "importing Jupyter notebook from Talk.ipynb\n",
      "importing Jupyter notebook from EncoderDecoder.ipynb\n",
      "importing Jupyter notebook from Trainer.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/carson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import *\n",
    "from Elements import * \n",
    "from Talk import *\n",
    "from Trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, emb_dim, dim_k = None, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_k = dim_k if dim_k else emb_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q_linear = nn.Linear(emb_dim,self.dim_k*num_heads)\n",
    "        self.k_linear = nn.Linear(emb_dim,self.dim_k*num_heads)\n",
    "        self.v_linear = nn.Linear(emb_dim,self.dim_k*num_heads)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(self.dim_k*num_heads,emb_dim)\n",
    "    \n",
    "    def attention(self, q, k, v, dim_k, mask=None, dropout=None, explain=False):\n",
    "        k = k.transpose(-2, -1)\n",
    "        if explain: print('q, k', q.shape, k.shape)\n",
    "        # matrix multiplication is done using the last two dimensions\n",
    "        # (batch_size,num_heads,q_seq_len,dim_k)X(batch_size,num_heads,dim_k,k_seq_len)\n",
    "        #(batch_size,num_heads,q_seq_len,k_seq_len)\n",
    "        scores = torch.matmul(q, k) / math.sqrt(dim_k) \n",
    "        if explain: print('scores.shape', scores.shape)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            if explain: print('mask.shape', mask.shape)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9) \n",
    "        softscores = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None: softscores = dropout(softscores)\n",
    "            \n",
    "        #(batch_size,num_heads,seq_len,seq_len)X(batch_size,num_heads,seq_len,dim_k)\n",
    "        output = torch.matmul(softscores, v)\n",
    "        return output, scores #=(batch_size,num_heads,seq_len,dim_k)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None, explain=False):\n",
    "        '''\n",
    "        inputs:\n",
    "            q has shape (batch size, q_sequence length, embedding dimensions)\n",
    "            k,v are shape (batch size, kv_sequence length, embedding dimensions)\n",
    "            source_mask of shape (batch size, 1, kv_sequence length)\n",
    "        outputs: sequence of vectors, re-represented using attention\n",
    "            shape (batch size, q_sequence length, embedding dimensions)\n",
    "        use:\n",
    "            The encoder layer places the same source vector sequence into q,k,v \n",
    "            and source_mask into mask.\n",
    "            The decoder layer uses this twice, once with decoder inputs as q,k,v \n",
    "            and target mask as mask. then with decoder inputs as q, encoder outputs\n",
    "            as k, v and source mask as mask\n",
    "        '''\n",
    "        # k,q,v are each shape (batch size, sequence length, dim_k * num_heads)\n",
    "        batch_size = q.size(0)\n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "        if explain: print(\"(batch size, sequence length, dim_k * num_heads)\", k.shape)\n",
    "        # k,q,v are each shape (batch size, sequence length, num_heads, dim_k)\n",
    "        k = k.view(batch_size,-1,self.num_heads,self.dim_k)\n",
    "        q = q.view(batch_size,-1,self.num_heads,self.dim_k)\n",
    "        v = v.view(batch_size,-1,self.num_heads,self.dim_k)\n",
    "        # transpose to shape (batch_size, num_heads, sequence length, dim_k)\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        if explain: print(\"(batch_size,num_heads,seq_length,dim_k)\",k.shape)\n",
    "        # calculate attention using function we will define next\n",
    "        attn, scores = self.attention(q, k, v, self.dim_k, mask, self.dropout, explain)\n",
    "        if explain: print(\"attn(batch_size,num_heads,seq_length,dim_k)\", attn.shape)\n",
    "        # concatenate heads and \n",
    "        concat=attn.transpose(1,2).contiguous().view(batch_size,-1,self.dim_k*self.num_heads)\n",
    "        if explain: print(\"concat.shape\", concat.shape)\n",
    "        # put through final linear layer\n",
    "        output = self.out(concat)\n",
    "        if explain: print(\"MultiHeadAttention output.shape\", output.shape)\n",
    "        return output, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(emb_dim)\n",
    "        self.norm_2 = Norm(emb_dim)\n",
    "        self.norm_3 = Norm(emb_dim)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, emb_dim, dropout=dropout)\n",
    "        self.attn_2 = MultiHeadAttention(heads, emb_dim, dropout=dropout)\n",
    "        self.ff = FeedForward(emb_dim, dropout=dropout)\n",
    "\n",
    "    def forward(self, fc_vecs, fc_mask, cn_vecs, cn_mask, explain = False):\n",
    "        '''\n",
    "        fc = focus, the sequence of vectors we are re-representing using the context\n",
    "        cn = context, the sequence of vectors that forms the context\n",
    "        inputs:\n",
    "            fc_vecs (batch size, fc_seq_len, emb_dim)\n",
    "            fc_mask (batch size, fc_seq_len, fc_seq_len)\n",
    "            cn_vecs (batch size, cn_seq_len, emb_dim)\n",
    "            cn_mask (batch size, 1, cn_sequence_len)\n",
    "        ouputs:\n",
    "            fc_vecs (batch size, fc_seq_len, emb_dim)\n",
    "        '''\n",
    "        fc_nrm = self.norm_1(fc_vecs)\n",
    "        #Self Attention \n",
    "        fc_attn, fc_scores = self.attn_1(fc_nrm,fc_nrm,fc_nrm,fc_mask,explain)\n",
    "        fc_vecs = fc_vecs + self.dropout_1(fc_attn)\n",
    "        fc_nrm = self.norm_2(fc_vecs)\n",
    "        #Context Attention \n",
    "        fc_attn, fc_scores = self.attn_2(fc_nrm,cn_vecs,cn_vecs,cn_mask,explain)\n",
    "        fc_vecs = fc_vecs + self.dropout_2(fc_attn)\n",
    "        fc_nrm = self.norm_3(fc_vecs)\n",
    "        fc_vecs = fc_vecs + self.dropout_3(self.ff(fc_nrm))\n",
    "        return fc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, n_layers, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = Embedder(vocab_size, emb_dim)\n",
    "        self.pe = PositionalEncoder(emb_dim, dropout=dropout)\n",
    "        self.layers = get_clones(ContextLayer(emb_dim, heads, dropout), n_layers)\n",
    "        self.norm = Norm(emb_dim)\n",
    "    def forward(self,  fc_toks, fc_mask, cn_vecs, cn_mask, explain = False):\n",
    "        '''\n",
    "        fc = focus, the sequence of vectors we are re-representing using the context\n",
    "        cn = context, the sequence of vectors that forms the context\n",
    "        inputs:\n",
    "            fc_toks (batch size, fc_seq_len)\n",
    "            fc_mask (batch size, fc_seq_len, fc_seq_len)\n",
    "            cn_vecs (batch size, cn_seq_len, emb_dim)\n",
    "            cn_mask (batch size, 1, cn_sequence_len)\n",
    "        ouputs:\n",
    "            fc_vecs (batch size, fc_seq_len, emb_dim)\n",
    "        '''\n",
    "        x = self.embed(fc_toks)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x, fc_mask, cn_vecs, cn_mask, explain)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, \n",
    "                 n_layers, num_heads, mem_slots, dropout):\n",
    "        \n",
    "        super().__init__() \n",
    "        \n",
    "        self.mem_slots = mem_slots\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_k = self.emb_dim // self.num_heads\n",
    "        self.batch_size = None \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.memory = torch.eye(self.mem_slots)\n",
    "        if self.emb_dim > self.mem_slots:\n",
    "          difference = self.emb_dim - self.mem_slots\n",
    "          pad = torch.zeros((self.mem_slots, difference))\n",
    "          self.memory = torch.cat([self.memory, pad], -1)\n",
    "        elif self.emb_dim < self.mem_slots:\n",
    "          self.memory = self.memory[:, :self.emb_dim]\n",
    "        \n",
    "        mem_mask = np.ones((1,self.mem_slots)).astype('uint8')\n",
    "        \n",
    "        self.mem_mask =  torch.from_numpy(mem_mask) == 1\n",
    "        \n",
    "        self.in_mem = Context(in_vocab_size, emb_dim, \n",
    "                              n_layers, num_heads, dropout)\n",
    "        \n",
    "        self.out_en = Context(out_vocab_size, emb_dim, \n",
    "                               n_layers, num_heads, dropout)\n",
    "        \n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "        \n",
    "        self.mem_update = MultiHeadAttention(self.num_heads, self.emb_dim, self.dim_k,\n",
    "                                             self.dropout)\n",
    "        \n",
    "    def batch_memory(self,src_seq):\n",
    "        self.batch_size = src_seq.size(0)\n",
    "        self.memory = torch.stack([self.memory for _ in range(self.batch_size)])\n",
    "        self.mem_mask = torch.stack([self.mem_mask for _ in range(self.batch_size)])\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, ou_toks, ou_mask, explain = False):\n",
    "        '''\n",
    "        in = input, the sequence we are encoding given the memory\n",
    "        ou = output, the sequence we are predicting the next action for\n",
    "             given the memory and input\n",
    "        inputs:\n",
    "            in_toks (batch size, in_seq_len)\n",
    "            in_mask (batch size, in_seq_len, in_seq_len)\n",
    "            ou_toks (batch size, ou_seq_len)\n",
    "            ou_mask (batch size, 1, ou_sequence_len)\n",
    "        ouputs:\n",
    "            ou_toks (batch size, ou_seq_len, out_vocab_size)\n",
    "        '''\n",
    "        if self.batch_size == None: self.batch_memory(in_toks)\n",
    "\n",
    "        en_vecs = self.in_mem(in_toks, in_mask, self.memory, self.mem_mask, explain)\n",
    "\n",
    "        ou_vecs = self.out_en(ou_toks, ou_mask, en_vecs, in_mask, explain)\n",
    "        \n",
    "        logits = self.out(ou_vecs)\n",
    "        \n",
    "        mem_dialogue = torch.cat([self.memory, en_vecs, ou_vecs], dim=-2) \n",
    "        \n",
    "        self.memory, scores = self.mem_update(self.memory,mem_dialogue,\n",
    "                                              mem_dialogue)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options(batchsize=1, device = torch.device(\"cpu\"), epochs=1, lr=0.01, \n",
    "              max_len = 20, save_path = '../saved/weights/memory_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../saved/pairs.json', opt=opt)\n",
    "\n",
    "emb_dim, n_layers, num_heads, mem_slots, dropout = 32, 1, 8, 4, 0.01 \n",
    "\n",
    "chloe = Memory(len(infield.vocab), len(outfield.vocab), \n",
    "               emb_dim, n_layers, num_heads, mem_slots, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_list = [\n",
    "    {\"listen\":\"my name is fluffy\", \"reply\":\"hello fluffy!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"its fluffy silly\"},\n",
    "    {\"listen\":\"my name is snuggles\", \"reply\":\"hello snuggles!\"},\n",
    "    {\"listen\":\"what is my name?\", \"reply\":\"its snuggles silly\"},\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 1.041\n",
      "0m: epoch 0 loss = 1.055\n",
      "0m: epoch 0 loss = 0.826\n",
      "0m: epoch 0 loss = 0.855\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(chloe.parameters(),lr=opt.lr,betas=(0.9, 0.98),eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',factor=0.9,patience=3)\n",
    "sos_tok = torch.LongTensor([[outfield.vocab.stoi['<sos>']]]) \n",
    "eos_tok = torch.LongTensor([[outfield.vocab.stoi['<eos>']]]) \n",
    "\n",
    "model.train()\n",
    "start = time.time()\n",
    "best_loss = 100\n",
    "for epoch in range(opt.epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(conversation_list)):\n",
    "        \n",
    "        listen_toks = string2tensor(conversation_list[i][\"listen\"], infield)\n",
    "        reply_toks = string2tensor(conversation_list[i][\"reply\"], infield)\n",
    "        reply_start = torch.cat((sos_tok,reply_toks), dim=1)\n",
    "        reply_labels = torch.cat((reply_toks,eos_tok), dim=1)\n",
    "        listen_mask, reply_mask = create_masks(listen_toks, reply_start, opt)\n",
    "        en_vecs = model.in_mem(listen_toks,listen_mask,model.memory,model.mem_mask)\n",
    "\n",
    "        for pos in range(opt.max_len):\n",
    "            \n",
    "            decoder_input = reply_start[:,:pos+1]\n",
    "            reply_labels_sofar = reply_labels[:,:pos+1].contiguous().view(-1)\n",
    "            \n",
    "            decoder_input_mask = nopeak_mask(size=pos+1, opt=opt) \n",
    "            \n",
    "            ou_vecs = model.out_en(decoder_input, decoder_input_mask, en_vecs, listen_mask)\n",
    "\n",
    "            mem_dialogue = torch.cat([model.memory, en_vecs, ou_vecs], dim=-2) \n",
    "\n",
    "            model.memory, scores = model.mem_update(model.memory,mem_dialogue,mem_dialogue)\n",
    "\n",
    "            out = model.out(ou_vecs)\n",
    "\n",
    "            flat_output = out.view(-1, out.size(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = F.cross_entropy(flat_output, reply_labels_sofar, \n",
    "                                         ignore_index = opt.trg_pad)\n",
    "            batch_loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "            epoch_loss = total_loss/len(conversation_list)\n",
    "            scheduler.step(epoch_loss)\n",
    "\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                torch.save(chloe.state_dict(), opt.save_path)\n",
    "            print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, \n",
    "                                                epoch, epoch_loss))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_talk(input_str, model, opt, infield, outfield):\n",
    "    '''\n",
    "    input:\n",
    "        input_str is a string, it is what you want to say to the dialogue model\n",
    "        model is a Transformer model with encoder, decoder and a last layer linear transformation\n",
    "        opt is an options object with the maximum length of the output sequence opt.max_len\n",
    "        infield and outfield are the data.fields that store the vocabulary\n",
    "    output:\n",
    "        an output string response from the dialogue model\n",
    "    \n",
    "    Note: this version assumes we are evaluating the model on CPU \n",
    "    '''\n",
    "    model.eval()\n",
    "    input_sequence = string2tensor(input_str, infield) # string to tensor \n",
    "    input_mask = (input_sequence != infield.vocab.stoi['<pad>']).unsqueeze(-2) #make input mask\n",
    "    # use the encoder rerepresent the input\n",
    "    en_vecs = model.in_mem(input_sequence, input_mask, model.memory, model.mem_mask)\n",
    "    init_tok = outfield.vocab.stoi['<sos>'] # this is the integer for the start token\n",
    "    decoder_input = torch.LongTensor([[init_tok]]) # use start token to initiate the decoder\n",
    "    logprobs = torch.Tensor([[]])\n",
    "    \n",
    "    # continue obtaining the next decoder token until decoder outputs and end token or til max_len\n",
    "    for pos in range(opt.max_len):\n",
    "        decoder_input_mask = nopeak_mask(size=pos+1, opt=opt) # make target mask, pos+1 casue pos starts at 0\n",
    "        # the out vector contains the logits that are rebalanced by the softmax\n",
    "        ou_vecs = model.out_en(decoder_input, decoder_input_mask, en_vecs, input_mask)\n",
    "        \n",
    "        mem_dialogue = torch.cat([model.memory, en_vecs, ou_vecs], dim=-2) \n",
    "        \n",
    "        model.memory, scores = model.mem_update(model.memory,mem_dialogue,mem_dialogue)\n",
    "        \n",
    "        out = model.out(ou_vecs)\n",
    "        \n",
    "        #softout is a categorical probability distribution over the output vocab\n",
    "        softout = F.softmax(out, dim=-1)\n",
    "        distr = Categorical(probs=softout)\n",
    "        action = distr.sample()[:,-1].unsqueeze(0)\n",
    "        logprob = -distr.log_prob(action)[:,-1].unsqueeze(0)\n",
    "        # concatenate that token to our running list of output tokens \n",
    "        decoder_input = torch.cat((decoder_input, action), dim=1)\n",
    "        logprobs = torch.cat((logprobs, logprob), dim=1)\n",
    "        # if the model outputs an end of sentence token, it is done with this sentence\n",
    "        if outfield.vocab.itos[action] == '<eos>':\n",
    "            # [0] because we are assuming batch size of 1 \n",
    "            # [1:-1] excludes the start and end token from the output string \n",
    "            de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "            return decoder_input, de_str, logprobs\n",
    "\n",
    "    de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "    return decoder_input, de_str, logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> hi name just <eos>\n"
     ]
    }
   ],
   "source": [
    "input_str = \"my name is fluffy\"\n",
    "decoder_input, de_str, logprobs = mem_talk(input_str, chloe, opt, infield, outfield)\n",
    "print(de_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
