{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Elements.ipynb\n",
      "importing Jupyter notebook from MoveData.ipynb\n"
     ]
    }
   ],
   "source": [
    "import math, copy, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import import_ipynb\n",
    "from Elements import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder \n",
    "\n",
    "All the subcomponents of the `EncoderLayer()` are thoroughly explained in Elements.ipynb\n",
    "\n",
    "the inputs to the encoder are sequences of integers shaped (batch size, sequence length)\n",
    "\n",
    "the output are sequences of vectors shaped (batch size, sequence length, embedding dimensions)\n",
    "\n",
    "<img src=\"../saved/images/encoderchart.png\">\n",
    "\n",
    "The diagram above mirrors the code below. As you follow the diagram upwards, you can see the data pass through the same modules as you move down the lines of code. `x` in our code is the sequence of tokens, the black line, moving through the diagram.\n",
    "\n",
    "The box labeled Nx is the `EncoderLayer` function below, that is repeated an arbitrary Nx number of times (6 in the paper). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(emb_dim)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.attn = MultiHeadAttention(heads, emb_dim, dropout=dropout)\n",
    "        self.norm_2 = Norm(emb_dim)\n",
    "        self.ff = FeedForward(emb_dim, dropout=dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x2_attn, x2_scores = self.attn(x2,x2,x2,mask)\n",
    "        x = x + self.dropout_1(x2_attn)\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward function of `Encoder` you can see that just as in the diagram, the embedding occurs first `x = self.embed(source_sequence)` followed by positional encoding `x = self.pe(x)`. Then the `EncoderLayer(emb_dim, heads, dropout)` is repeated an `n_layers` number of times. \n",
    "\n",
    "Within the gray box in the diagram you can see the first step is the Multi-Head Attention. In our implementation we do a normalization first. But the the data splits into 3 arrows before going into Multi-Head Attention, just like we use the `x2` three times in `x2_attn, x2_scores = self.attn(x2,x2,x2,mask)`. This signifies that we used the same sequence of vectors for the generation of `num_heads` number of query `q`, key `k` and  value `v` vectors. \n",
    "\n",
    "Another minor difference is the use of dropout. Dropout is the random zeroing out of certain neurons or activations. The line above that reads `x = x + self.dropout_1(x2_attn)` could be written `x = x + x2_attn` to make it resemble the residual connection in the diagram. The line in the diagram that branches and goes around the Multi-Head Attention layer and into the \"Add & Norm\" module, is the residual. In english, it means that the input to the norm layer is the sum of x after it passes through the Multi-Head Attention with a copy of x that has not passed through the Multi-Head Attention.\n",
    "\n",
    "In the same way, the output of the feed forward layer `self.ff(x2)` is added to itself before normalized again. This is the 2nd residual in the diagram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_layers, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = Embedder(vocab_size, emb_dim)\n",
    "        self.pe = PositionalEncoder(emb_dim, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(emb_dim, heads, dropout), n_layers)\n",
    "        self.norm = Norm(emb_dim)\n",
    "    def forward(self, source_sequence, source_mask):\n",
    "        '''\n",
    "        input:\n",
    "            source_sequence (sequence of source tokens) of shape (batch size, sequence length)\n",
    "            source_mask (mask over input sequence) of shape (batch size, 1, sequence length)\n",
    "        output: sequence of vectors after embedding, postional encoding, attention and normalization\n",
    "            shape (batch size, sequence length, embedding dimensions)\n",
    "        '''\n",
    "        x = self.embed(source_sequence)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x, source_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
