{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of creating a playground-like teaching and learning environment, all the code used here is stored in a ipython or jupyter notebook. import_ipynb is used to import classes and functions from other notebooks. Since these imports will run all the cells in the notebook you are importing from, once you have finished playing with a certain module or cell in a notebook, comment out the cells you would rather not have executed when the notebook is imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /media/carson/New Volume/Chloe/chloebot/notebooks/Data_mover.ipynb\n",
      "importing Jupyter notebook from /media/carson/New Volume/Chloe/chloebot/notebooks/Encoder.ipynb\n",
      "importing Jupyter notebook from /media/carson/New Volume/Chloe/chloebot/notebooks/Decoder.ipynb\n",
      "importing Jupyter notebook from /media/carson/New Volume/Chloe/chloebot/notebooks/Trainer.ipynb\n"
     ]
    }
   ],
   "source": [
    "import math, copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from notebooks.Data_mover import csv2datatools, Options, num_batches\n",
    "from notebooks.Encoder import Encoder \n",
    "from notebooks.Decoder import Decoder \n",
    "from notebooks.Trainer import trainer, CosineWithRestarts\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options(batchsize = 4)\n",
    "data_iter, infield, outfield, opt = csv2datatools('saved/translation_pairs.csv','en', opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "    def forward(self, src_seq, trg_seq, src_mask, trg_mask):\n",
    "        e_output = self.encoder(src_seq, src_mask)\n",
    "        d_output = self.decoder(trg_seq, e_output, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim, n_layers, heads, dropout = 64, 2, 8, 0.1 \n",
    "opt.save_path = 'saved/weights/model_weights'\n",
    "model = Transformer(len(infield.vocab), len(outfield.vocab), emb_dim, n_layers, heads, dropout)\n",
    "if opt.device != -1:\n",
    "    model = model.cuda()\n",
    "model.load_state_dict(torch.load(opt.save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr = 0.001 #0.0001\n",
    "opt.epochs = 20 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = CosineWithRestarts(optimizer, T_max=num_batches(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 0.004\n",
      "0m: epoch 1 loss = 0.006\n",
      "0m: epoch 2 loss = 0.004\n",
      "0m: epoch 3 loss = 0.005\n",
      "0m: epoch 4 loss = 0.003\n",
      "0m: epoch 5 loss = 0.003\n",
      "0m: epoch 6 loss = 0.003\n",
      "0m: epoch 7 loss = 0.003\n",
      "0m: epoch 8 loss = 0.003\n",
      "0m: epoch 9 loss = 0.003\n",
      "0m: epoch 10 loss = 0.003\n",
      "0m: epoch 11 loss = 0.002\n",
      "0m: epoch 12 loss = 0.002\n",
      "0m: epoch 13 loss = 0.002\n",
      "0m: epoch 14 loss = 0.002\n",
      "0m: epoch 15 loss = 0.002\n",
      "0m: epoch 16 loss = 0.002\n",
      "0m: epoch 17 loss = 0.002\n",
      "0m: epoch 18 loss = 0.001\n",
      "0m: epoch 19 loss = 0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedder(\n",
       "      (embed): Embedding(19, 64)\n",
       "    )\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): Embedder(\n",
       "      (embed): Embedding(46, 64)\n",
       "    )\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (out): Linear(in_features=64, out_features=46, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = trainer(model, data_iter, opt, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
