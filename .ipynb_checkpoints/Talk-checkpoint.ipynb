{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_model(sentence, model, opt, SRC, TRG):\n",
    "    model.eval()\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "    if opt.device != -1:\n",
    "        sentence = sentence.cuda()\n",
    "    #print('sentence.is_cuda',sentence.is_cuda)\n",
    "    sentence = beam_search(sentence, model, SRC, TRG, opt)\n",
    "    return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonym(word, SRC):\n",
    "    syns = wordnet.synsets(word)\n",
    "    for s in syns:\n",
    "        for l in s.lemmas():\n",
    "            if SRC.vocab.stoi[l.name()] != 0:\n",
    "                return SRC.vocab.stoi[l.name()]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(src, model, SRC, TRG, opt):\n",
    "    #print(next(model.parameters()).is_cuda,src.is_cuda)\n",
    "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, opt)\n",
    "    #e_outputs.shape [batch_size, seq_len, emd_dim] print(outputs, e_outputs.shape, log_scores)\n",
    "    eos_tok = TRG.vocab.stoi['<eos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    ind = None\n",
    "    for i in range(2, opt.max_len):\n",
    "        trg_mask = nopeak_mask(i, opt)\n",
    "        out = model.out(model.decoder(outputs[:,:i], e_outputs, src_mask, trg_mask))\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, opt.k)\n",
    "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
    "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
    "        for vec in ones:\n",
    "            i = vec[0]\n",
    "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
    "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
    "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
    "        if num_finished_sentences == opt.k:\n",
    "            alpha = 0.7\n",
    "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
    "            _, ind = torch.max(log_scores * div, 1)\n",
    "            ind = ind.data[0]\n",
    "            break\n",
    "    if ind is None:\n",
    "        print(outputs[0]==eos_tok)\n",
    "        print((outputs[0]==eos_tok).nonzero())\n",
    "        length = (outputs[0]==eos_tok).nonzero()[0]\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "    else:\n",
    "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_best_outputs(outputs, out, log_scores, i, k):\n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
    "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "    outputs[:, :i] = outputs[row, :i]\n",
    "    outputs[:, i] = ix[row, col]\n",
    "    log_scores = k_probs.unsqueeze(0)\n",
    "    return outputs, log_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_talk(sentence, model, opt, SRC, TRG, max_len):\n",
    "    model.eval()\n",
    "    input_seq = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    #print(sentence)\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0:\n",
    "            input_seq.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            input_seq.append(get_synonym(tok, SRC))\n",
    "    input_seq = torch.LongTensor([input_seq])\n",
    "    if opt.device != -1:\n",
    "        input_seq = input_seq.cuda()\n",
    "    #print(input_seq)\n",
    "    encoding_mask = (input_seq != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    #print(encoding_mask)\n",
    "    encoding = model.encoder(input_seq, encoding_mask)\n",
    "    #print(encoding.shape)\n",
    "    start_index = outfield.vocab.stoi['<sos>']\n",
    "    ys = torch.ones(1, 1).fill_(start_index).type_as(input_seq.data)\n",
    "    #print(ys)\n",
    "    for i in range(max_len-1):\n",
    "        np_mask = nopeak_mask(ys.size(1), opt).type_as(input_seq.data)\n",
    "        print(encoding.shape, encoding_mask.shape, ys.shape, np_mask.shape)\n",
    "        out = model.decoder(encoding, encoding_mask, ys, np_mask)\n",
    "        break \n",
    "    #print('sentence.is_cuda',sentence.is_cuda)\n",
    "    #sentence = beam_search(sentence, model, SRC, TRG, opt)\n",
    "    # sentence = multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size, opt):\n",
    "    \"Mask out subsequent positions. aka subsequent_mask\"\n",
    "    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "    np_mask =  torch.from_numpy(np_mask) == 0\n",
    "    if opt.device != -1:\n",
    "      np_mask = np_mask.cuda()\n",
    "    return np_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vars(src, model, SRC, TRG, opt):\n",
    "    init_tok = TRG.vocab.stoi['<sos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    #print(next(model.parameters()).is_cuda,src.is_cuda,src_mask.is_cuda)\n",
    "    e_output = model.encoder(src, src_mask)\n",
    "    outputs = torch.LongTensor([[init_tok]])\n",
    "    if opt.device != -1:\n",
    "        outputs = outputs.cuda()\n",
    "    trg_mask = nopeak_mask(1, opt)\n",
    "    out = model.out(model.decoder(outputs, e_output, src_mask, trg_mask))\n",
    "    out = F.softmax(out, dim=-1)\n",
    "    #print(out.shape)\n",
    "    probs, ix = out[:, -1].data.topk(opt.k)\n",
    "    #print('probs, ix',probs, ix)\n",
    "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
    "    #print(log_scores)\n",
    "    outputs = torch.zeros(opt.k, opt.max_len).long()\n",
    "    if opt.device != -1:\n",
    "        outputs = outputs.cuda()\n",
    "    outputs[:, 0] = init_tok\n",
    "    outputs[:, 1] = ix[0]\n",
    "    e_outputs = torch.zeros(opt.k, e_output.size(-2),e_output.size(-1))\n",
    "    if opt.device != -1:\n",
    "        e_outputs = e_outputs.cuda()\n",
    "    e_outputs[:, :] = e_output[0]\n",
    "    return outputs, e_outputs, log_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encoder(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, ys, \n",
    "                           subsequent_mask(ys.size(1)).type_as(src.data))\n",
    "        prob = model.out(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(dict, text):\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
