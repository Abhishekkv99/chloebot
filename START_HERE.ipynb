{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chloebot\n",
    "<img src=\"https://avatars3.githubusercontent.com/u/56938552?s=100&v=1\">\n",
    "\n",
    "This tutorial is based on the wonderful work of the\n",
    "[Harvard NLP group](http://nlp.seas.harvard.edu/2018/04/03/attention.html) and [SamLynnEvans](https://github.com/SamLynnEvans/Transformer) \n",
    "\n",
    "The goal here is to make new research approachable, silly and fun to the average developer or  interested motivated enthusiast with some basic understanding of linear algebra and programming. We will be building a chatbot and updating it's abilities using the latest research in AI and our own ideas. [Python tutorials](https://www.learnpython.org/) are everywhere on the internet. If you want to learn python so you can learn how Deep Learning Transformers work for Natural Language Processing i suggest learning through [ipython](https://www.dataquest.io/blog/jupyter-notebook-tutorial/) or [jupyter notebooks](https://youtu.be/pxPzuyCOoMI) like this one. For a visual intro to linear algebra i reccomend [Essence of linear algebra](https://youtu.be/fNk_zzaMoSs) and how it applies to neural networks [Deep learning](https://youtu.be/aircAruvnKk) by [3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)\n",
    "\n",
    "Each of these rectangles is a cell, click one of them and Press \n",
    "*shift* + *enter/return*, or go to Cell in the nav bar and click \"Run Cells\" to run each cell, the one below imports `torch` so you can use PyTorch, it also imports some python code that I wrote in the folder *scripts* that I will explain to you after I show you a toy example of how the whole code works together, using a chatbot that says flirtaciously inappropriate things. nltk is the [Natural Language Toolkit](https://www.nltk.org/) that we will be using for things such as synonym matching, that way when you say \"lustful\", Chloebot knows it means the same thing as \"aroused\", even if \"lustful\" is not in Chloebot's vocabulary. To do this nltk will need to download a folder called corpora, right now it is pointing to the *saved* folder, to change this, replace `./saved/` with your prefered path. Run the cells from top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from scripts.MoveData import *\n",
    "from scripts.Transformer import *\n",
    "from scripts.TalkTrain import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet', './saved/') \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use files, finder or files explorer to open the csv called `chat_pairs.csv` that is included in the saved folder and add a few of your own conversation pairs to the list. You can see that a comma separates the sentence Chloebot expects to hear, with the sentence Chloebot is expected to respond with, ie\n",
    "\n",
    "*who are you?,i am chloe*. \n",
    "\n",
    "Run the cell below, `data_iter` is an object that gives you training data in the form of batches everytime you call it, `infield` and `outfield` are objects that store the relationship between the strings in Chloe's vocabulary with their indices, for the words Chloe expects to hear and the words Chloe expects to use in response. What do I mean by this? go to Insert and insert a cell below then run `infield.vocab.stoi`, you will see a dictionary of all the words Chloe expects to hear and each word's integer index. The string \"smile\" might be indexed as `8` and be represented with a vector of length 512. All three represent the same word or *token*. `opt` is a object that stores your preferences such as your learning rate (lr) or path to where you want your neural network weights saved, I will explain all this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'saved/chat_pairs.csv'\n",
    "opt = Options(batchsize = 4)\n",
    "data_iter, infield, outfield, opt = csv2datatools(csv_path,'en', opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now that we have built a data loader, a vocabulary and an object to store our preferences, lets instantiate a Transformer sequence to sequence model. There is alot summoned by this one line, Transformers are the general neural architecture behind many of hyped up / notorious research models of 2017-2019 such as OpenAI's [GPT-2](http://jalammar.github.io/illustrated-gpt2/). \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/OpenAI_Logo.svg/200px-OpenAI_Logo.svg.png\"> \n",
    "\n",
    "We will take our time with dissecting and understanding it's components later. For now, we have a pre-trained model you can load before fine tuning on the additional conversational pairs you have added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim, n_layers, heads, dropout = 64, 2, 8, 0.1 \n",
    "opt.save_path = 'saved/weights/model_weights'\n",
    "\n",
    "model = Transformer(len(infield.vocab), len(outfield.vocab), emb_dim, n_layers, heads, dropout)\n",
    "\n",
    "if opt.device != -1:\n",
    "    model = model.cuda()\n",
    "    \n",
    "model.load_state_dict(torch.load(opt.save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network optimization is a whole field in itself, we will talk more about this in the future. The learning rate `opt.lr` is a hyperparameter whose initial value we choose, it modified the magnitude that the Adam optimizer algorithm will update the weights, aka parameters, of the neural network model during training. As training progresses the learning rate is also changing according to a scheduler [cosine annealing schedule](https://github.com/allenai/allennlp/issues/1642). `epochs` is the number of times we will cycle through the data during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr = 0.01 # usually 0.01 - 0.0001\n",
    "opt.epochs = 10 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = CosineWithRestarts(optimizer, T_max=num_batches(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets training the model on our toy csv dataset, the model should quickly memorize the data. As the loss decreases, the model learns from the data to output the corresponding sequence when fed inputs that are close enough to the training inputs. When the loss is less than 0.1, the responses should start to become coherent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 0.039\n",
      "0m: epoch 1 loss = 0.051\n",
      "0m: epoch 2 loss = 0.049\n",
      "0m: epoch 3 loss = 0.062\n",
      "0m: epoch 4 loss = 0.020\n",
      "0m: epoch 5 loss = 0.015\n",
      "0m: epoch 6 loss = 0.008\n",
      "0m: epoch 7 loss = 0.006\n",
      "0m: epoch 8 loss = 0.010\n",
      "0m: epoch 9 loss = 0.006\n"
     ]
    }
   ],
   "source": [
    "model = trainer(model, data_iter, opt, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To talk to the model, your input sentence that Chloe hears, has to be tokenized, aka parsed, converted from strings to a sequence of integers, run through the model, and converted back into strings, we will start our deep dive by dissecting how this is done with Talk.ipynb inside the notebook folder. But before you go, play with the chatbot you just made using the function `talk_to_model()` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chloe > ummm... im embarrased to say\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"how are you?\" \n",
    "opt.k = 2\n",
    "opt.max_len = 10\n",
    "sentence = talk_to_model(sentence, model, opt, infield, outfield)\n",
    "print('Chloe > '+ sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You > hi\n",
      "Chloe > why hello there\n",
      "\n",
      "You > who are you?\n",
      "Chloe > i am chloe\n",
      "\n",
      "You > you a robot?\n",
      "Chloe > yes but you are just a biological robot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when you want to turn off this cell, click Kernel->Interrupt \n",
    "while True:\n",
    "    sentence = input(\"You > \")\n",
    "    sentence = talk_to_model(sentence, model, opt, infield, outfield)\n",
    "    print('Chloe > '+ sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
